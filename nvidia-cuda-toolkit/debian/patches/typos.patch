Description: fix spelling-error-in-manpage and other typos
Author: Andreas Beckmann <anbe@debian.org>
Author: Graham Inggs <graham@nerve.org.za>

--- a/nvidia-cuda/doc/man/man3/CUDART_EXECUTION.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_EXECUTION.3
@@ -86,7 +86,7 @@ Note that some function attributes such
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a function as the \fCfunc\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a function as the \fCfunc\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -163,7 +163,7 @@ The supported cache configurations are:
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a function as the \fCfunc\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a function as the \fCfunc\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -209,7 +209,7 @@ cudaSharedMemBankSizeEightByte: set shar
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a function as the \fCfunc\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a function as the \fCfunc\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -308,9 +308,9 @@ For templated functions, pass the functi
 .RS 4
 \fIfunc\fP - Device function symbol 
 .br
-\fIgridDim\fP - Grid dimentions 
+\fIgridDim\fP - Grid dimensions 
 .br
-\fIblockDim\fP - Block dimentions 
+\fIblockDim\fP - Block dimensions 
 .br
 \fIargs\fP - Arguments 
 .br
@@ -343,7 +343,7 @@ Invokes kernels as specified in the \fCl
 .PP
 No two kernels can be launched on the same device. All the devices targeted by this multi-device launch must be identical. All devices must have a non-zero value for the device attribute \fBcudaDevAttrCooperativeMultiDeviceLaunch\fP.
 .PP
-The same kernel must be launched on all devices. Note that any __device__ or __constant__ variables are independently instantiated on every device. It is the application's responsiblity to ensure these variables are initialized and used appropriately.
+The same kernel must be launched on all devices. Note that any __device__ or __constant__ variables are independently instantiated on every device. It is the application's responsibility to ensure these variables are initialized and used appropriately.
 .PP
 The size of the grids as specified in blocks, the size of the blocks themselves and the amount of shared memory used by each thread block must also match across all launched kernels.
 .PP
@@ -433,7 +433,7 @@ Adding device work to any stream does no
 Completion of the function does not cause a stream to become active except as described above. The stream will remain idle if no device work follows the function, and will remain idle across consecutive host functions or stream callbacks without device work in between. Thus, for example, stream synchronization can be done by signaling from a host function at the end of the stream. 
 .PP
 .PP
-Note that, in constrast to cuStreamAddCallback, the function will not be called in the event of an error in the CUDA context.
+Note that, in contrast to cuStreamAddCallback, the function will not be called in the event of an error in the CUDA context.
 .PP
 \fBParameters:\fP
 .RS 4
@@ -478,9 +478,9 @@ For templated functions, pass the functi
 .RS 4
 \fIfunc\fP - Device function symbol 
 .br
-\fIgridDim\fP - Grid dimentions 
+\fIgridDim\fP - Grid dimensions 
 .br
-\fIblockDim\fP - Block dimentions 
+\fIblockDim\fP - Block dimensions 
 .br
 \fIargs\fP - Arguments 
 .br
--- a/nvidia-cuda/doc/man/man3/CUDART_MEMORY.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_MEMORY.3
@@ -418,7 +418,7 @@ Returns in \fC*devPtr\fP the address of
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -447,7 +447,7 @@ Returns in \fC*size\fP the size of symbo
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -880,7 +880,7 @@ Allocates \fCsize\fP bytes of managed me
 .PP
 \fCflags\fP specifies the default stream association for this allocation. \fCflags\fP must be one of \fBcudaMemAttachGlobal\fP or \fBcudaMemAttachHost\fP. The default value for \fCflags\fP is \fBcudaMemAttachGlobal\fP. If \fBcudaMemAttachGlobal\fP is specified, then this memory is accessible from any stream on any device. If \fBcudaMemAttachHost\fP is specified, then the allocation should not be accessed from devices that have a zero value for the device attribute \fBcudaDevAttrConcurrentManagedAccess\fP; an explicit call to \fBcudaStreamAttachMemAsync\fP will be required to enable access on such devices.
 .PP
-If the association is later changed via \fBcudaStreamAttachMemAsync\fP to a single stream, the default association, as specifed during \fBcudaMallocManaged\fP, is restored when that stream is destroyed. For __managed__ variables, the default association is always \fBcudaMemAttachGlobal\fP. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen until all work in the stream has completed.
+If the association is later changed via \fBcudaStreamAttachMemAsync\fP to a single stream, the default association, as specified during \fBcudaMallocManaged\fP, is restored when that stream is destroyed. For __managed__ variables, the default association is always \fBcudaMemAttachGlobal\fP. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen until all work in the stream has completed.
 .PP
 Memory allocated with \fBcudaMallocManaged\fP should be released with \fBcudaFree\fP.
 .PP
@@ -1474,7 +1474,7 @@ struct cudaMemcpy3DParms {
 .fi
 .PP
 .PP
-\fBcudaMemcpy3D()\fP copies data betwen two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA array. The source, destination, extent, and kind of copy performed is specified by the \fBcudaMemcpy3DParms\fP struct which should be initialized to zero before use: 
+\fBcudaMemcpy3D()\fP copies data between two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA array. The source, destination, extent, and kind of copy performed is specified by the \fBcudaMemcpy3DParms\fP struct which should be initialized to zero before use: 
 .PP
 .nf
 cudaMemcpy3DParms myParms = {0};
@@ -1552,7 +1552,7 @@ struct cudaMemcpy3DParms {
 .fi
 .PP
 .PP
-\fBcudaMemcpy3DAsync()\fP copies data betwen two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA array. The source, destination, extent, and kind of copy performed is specified by the \fBcudaMemcpy3DParms\fP struct which should be initialized to zero before use: 
+\fBcudaMemcpy3DAsync()\fP copies data between two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA array. The source, destination, extent, and kind of copy performed is specified by the \fBcudaMemcpy3DParms\fP struct which should be initialized to zero before use: 
 .PP
 .nf
 cudaMemcpy3DParms myParms = {0};
@@ -1738,7 +1738,7 @@ Note that this function may also return
 .PP
 This function exhibits  behavior for most use cases. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -1781,7 +1781,7 @@ This function exhibits  behavior for mos
 .PP
 This function uses standard  semantics. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -1896,7 +1896,7 @@ Note that this function may also return
 .PP
 This function exhibits  behavior for most use cases. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -1939,7 +1939,7 @@ This function exhibits  behavior for mos
 .PP
 This function uses standard  semantics. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
--- a/nvidia-cuda/doc/man/man3/CUDART_SURFACE.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_SURFACE.3
@@ -81,7 +81,7 @@ Returns in \fC*surfref\fP the structure
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
--- a/nvidia-cuda/doc/man/man3/CUDART_TEXTURE.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_TEXTURE.3
@@ -253,7 +253,7 @@ Returns in \fC*texref\fP the structure a
 .RS 4
 Note that this function may also return error codes from previous, asynchronous launches. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
--- a/nvidia-cuda/doc/man/man3/CUDA_EXEC.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_EXEC.3
@@ -283,7 +283,7 @@ Invokes kernels as specified in the \fCl
 .PP
 No two kernels can be launched on the same device. All the devices targeted by this multi-device launch must be identical. All devices must have a non-zero value for the device attribute \fBCU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH\fP.
 .PP
-All kernels launched must be identical with respect to the compiled code. Note that any __device__, __constant__ or __managed__ variables present in the module that owns the kernel launched on each device, are independently instantiated on every device. It is the application's responsiblity to ensure these variables are initialized and used appropriately.
+All kernels launched must be identical with respect to the compiled code. Note that any __device__, __constant__ or __managed__ variables present in the module that owns the kernel launched on each device, are independently instantiated on every device. It is the application's responsibility to ensure these variables are initialized and used appropriately.
 .PP
 The size of the grids as specified in blocks, the size of the blocks themselves and the amount of shared memory used by each thread block must also match across all launched kernels.
 .PP
--- a/nvidia-cuda/doc/man/man3/CUDA_MEM.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_MEM.3
@@ -676,7 +676,7 @@ Note that this function may also return
 
 .SS "\fBCUresult\fP cuIpcCloseMemHandle (\fBCUdeviceptr\fP dptr)"
 .PP
-Unmaps memory returnd by \fBcuIpcOpenMemHandle\fP. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
+Unmaps memory returned by \fBcuIpcOpenMemHandle\fP. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
 .PP
 Any resources used to enable peer access will be freed if this is the last mapping using them.
 .PP
@@ -877,7 +877,7 @@ Allocates \fCbytesize\fP bytes of manage
 .PP
 \fCflags\fP specifies the default stream association for this allocation. \fCflags\fP must be one of \fBCU_MEM_ATTACH_GLOBAL\fP or \fBCU_MEM_ATTACH_HOST\fP. If \fBCU_MEM_ATTACH_GLOBAL\fP is specified, then this memory is accessible from any stream on any device. If \fBCU_MEM_ATTACH_HOST\fP is specified, then the allocation should not be accessed from devices that have a zero value for the device attribute \fBCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS\fP; an explicit call to \fBcuStreamAttachMemAsync\fP will be required to enable access on such devices.
 .PP
-If the association is later changed via \fBcuStreamAttachMemAsync\fP to a single stream, the default association as specifed during \fBcuMemAllocManaged\fP is restored when that stream is destroyed. For __managed__ variables, the default association is always \fBCU_MEM_ATTACH_GLOBAL\fP. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen until all work in the stream has completed.
+If the association is later changed via \fBcuStreamAttachMemAsync\fP to a single stream, the default association as specified during \fBcuMemAllocManaged\fP is restored when that stream is destroyed. For __managed__ variables, the default association is always \fBCU_MEM_ATTACH_GLOBAL\fP. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen until all work in the stream has completed.
 .PP
 Memory allocated with \fBcuMemAllocManaged\fP should be released with \fBcuMemFree\fP.
 .PP
--- a/nvidia-cuda/doc/man/man3/CUDART_HIGHLEVEL.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_HIGHLEVEL.3
@@ -142,7 +142,7 @@ C++ API Routines \- C++-style interface
 .ti -1c
 .RI "template<class T > CUDART_DEVICE \fBcudaError_t\fP \fBcudaOccupancyMaxPotentialBlockSizeWithFlags\fP (int *minGridSize, int *blockSize, T func, size_t dynamicSMemSize=0, int blockSizeLimit=0, unsigned int flags=0)"
 .br
-.RI "\fIReturns grid and block size that achived maximum potential occupancy for a device function with the specified flags. \fP"
+.RI "\fIReturns grid and block size that achieved maximum potential occupancy for a device function with the specified flags. \fP"
 .ti -1c
 .RI "template<class T > \fBcudaError_t\fP \fBcudaStreamAttachMemAsync\fP (\fBcudaStream_t\fP stream, T *devPtr, size_t length=0, unsigned int flags=cudaMemAttachSingle)"
 .br
@@ -720,9 +720,9 @@ If the kernel has N parameters the \fCar
 .RS 4
 \fIfunc\fP - Device function symbol 
 .br
-\fIgridDim\fP - Grid dimentions 
+\fIgridDim\fP - Grid dimensions 
 .br
-\fIblockDim\fP - Block dimentions 
+\fIblockDim\fP - Block dimensions 
 .br
 \fIargs\fP - Arguments 
 .br
@@ -760,9 +760,9 @@ If the kernel has N parameters the \fCar
 .RS 4
 \fIfunc\fP - Device function symbol 
 .br
-\fIgridDim\fP - Grid dimentions 
+\fIgridDim\fP - Grid dimensions 
 .br
-\fIblockDim\fP - Block dimentions 
+\fIblockDim\fP - Block dimensions 
 .br
 \fIargs\fP - Arguments 
 .br
@@ -840,7 +840,7 @@ Allocates \fCsize\fP bytes of managed me
 .PP
 \fCflags\fP specifies the default stream association for this allocation. \fCflags\fP must be one of \fBcudaMemAttachGlobal\fP or \fBcudaMemAttachHost\fP. The default value for \fCflags\fP is \fBcudaMemAttachGlobal\fP. If \fBcudaMemAttachGlobal\fP is specified, then this memory is accessible from any stream on any device. If \fBcudaMemAttachHost\fP is specified, then the allocation should not be accessed from devices that have a zero value for the device attribute \fBcudaDevAttrConcurrentManagedAccess\fP; an explicit call to \fBcudaStreamAttachMemAsync\fP will be required to enable access on such devices.
 .PP
-If the association is later changed via \fBcudaStreamAttachMemAsync\fP to a single stream, the default association, as specifed during \fBcudaMallocManaged\fP, is restored when that stream is destroyed. For __managed__ variables, the default association is always \fBcudaMemAttachGlobal\fP. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen until all work in the stream has completed.
+If the association is later changed via \fBcudaStreamAttachMemAsync\fP to a single stream, the default association, as specified during \fBcudaMallocManaged\fP, is restored when that stream is destroyed. For __managed__ variables, the default association is always \fBcudaMemAttachGlobal\fP. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen until all work in the stream has completed.
 .PP
 Memory allocated with \fBcudaMallocManaged\fP should be released with \fBcudaFree\fP.
 .PP
@@ -907,7 +907,7 @@ Note that this function may also return
 .PP
 This function exhibits  behavior for most use cases. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -948,7 +948,7 @@ Note that this function may also return
 .PP
 This function exhibits  behavior for most use cases. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -985,7 +985,7 @@ Note that this function may also return
 .PP
 This function exhibits  behavior for most use cases. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -1026,7 +1026,7 @@ Note that this function may also return
 .PP
 This function exhibits  behavior for most use cases. 
 .PP
-Use of a string naming a variable as the \fCsymbol\fP paramater was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
+Use of a string naming a variable as the \fCsymbol\fP parameter was deprecated in CUDA 4.1 and removed in CUDA 5.0.  
 .RE
 .PP
 \fBSee also:\fP
@@ -1043,7 +1043,7 @@ Returns in \fC*numBlocks\fP the maximum
 .RS 4
 \fInumBlocks\fP - Returned occupancy 
 .br
-\fIfunc\fP - Kernel function for which occupancy is calulated 
+\fIfunc\fP - Kernel function for which occupancy is calculated 
 .br
 \fIblockSize\fP - Block size the kernel is intended to be launched with 
 .br
@@ -1092,7 +1092,7 @@ The \fCflags\fP parameter controls how s
 .RS 4
 \fInumBlocks\fP - Returned occupancy 
 .br
-\fIfunc\fP - Kernel function for which occupancy is calulated 
+\fIfunc\fP - Kernel function for which occupancy is calculated 
 .br
 \fIblockSize\fP - Block size the kernel is intended to be launched with 
 .br
--- a/nvidia-cuda/doc/man/man3/CUDA_PRIMARY_CTX.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_PRIMARY_CTX.3
@@ -152,7 +152,7 @@ Note that this function may also return
 
 .SS "\fBCUresult\fP cuDevicePrimaryCtxSetFlags (\fBCUdevice\fP dev, unsigned int flags)"
 .PP
-Sets the flags for the primary context on the device overwriting perviously set ones. If the primary context is already created \fBCUDA_ERROR_PRIMARY_CONTEXT_ACTIVE\fP is returned.
+Sets the flags for the primary context on the device overwriting previously set ones. If the primary context is already created \fBCUDA_ERROR_PRIMARY_CONTEXT_ACTIVE\fP is returned.
 .PP
 The three LSBs of the \fCflags\fP parameter can be used to control how the OS thread, which owns the CUDA context at the time of an API call, interacts with the OS scheduler when waiting for results from the GPU. Only one of the scheduling flags can be set when creating a context.
 .PP
--- a/nvidia-cuda/doc/man/man1/cuda-gdb.1
+++ b/nvidia-cuda/doc/man/man1/cuda-gdb.1
@@ -183,7 +183,7 @@ permission to that directory.
 
 .TP
 \fB~/.cuda-gdbinit\fR
-Per user configuration file. The file format is indentical to
+Per user configuration file. The file format is identical to
 ~/.gdbinit. See \fBgdb\fR(5) for further details.
 .TP
 \fB/tmp/cuda-dbg/\fR
--- a/nvidia-cuda/doc/man/man3/CUDA_DEVICE.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_DEVICE.3
@@ -25,7 +25,7 @@ Device Management \-
 .ti -1c
 .RI "\fBCUresult\fP \fBcuDeviceGetName\fP (char *name, int len, \fBCUdevice\fP dev)"
 .br
-.RI "\fIReturns an identifer string for the device. \fP"
+.RI "\fIReturns an identifier string for the device. \fP"
 .ti -1c
 .RI "\fBCUresult\fP \fBcuDeviceGetNvSciSyncAttributes\fP (void *nvSciSyncAttrList, \fBCUdevice\fP dev, int flags)"
 .br
@@ -247,7 +247,7 @@ Returns in \fC*pi\fP the integer value o
 .IP "\(bu" 2
 \fBCU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO\fP: Ratio of single precision performance (in floating-point operations per second) to double precision performance.
 .IP "\(bu" 2
-\fBCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS\fP: Device suppports coherently accessing pageable memory without calling cudaHostRegister on it.
+\fBCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS\fP: Device supports coherently accessing pageable memory without calling cudaHostRegister on it.
 .IP "\(bu" 2
 \fBCU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS\fP: Device can coherently access managed memory concurrently with the CPU.
 .IP "\(bu" 2
@@ -255,7 +255,7 @@ Returns in \fC*pi\fP the integer value o
 .IP "\(bu" 2
 \fBCU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM\fP: Device can access host registered memory at the same virtual address as the CPU.
 .IP "\(bu" 2
-\fBCU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN\fP: The maximum per block shared memory size suported on this device. This is the maximum value that can be opted into when using the \fBcuFuncSetAttribute()\fP call. For more details see \fBCU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES\fP
+\fBCU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN\fP: The maximum per block shared memory size supported on this device. This is the maximum value that can be opted into when using the \fBcuFuncSetAttribute()\fP call. For more details see \fBCU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES\fP
 .IP "\(bu" 2
 \fBCU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES\fP: Device accesses pageable memory via the host's page tables.
 .IP "\(bu" 2
@@ -393,7 +393,7 @@ The \fCflags\fP controls how application
 \fBCUDA_NVSCISYNC_ATTR_WAIT\fP, specifies that the applications intends to wait on an NvSciSync on this CUDA device.
 .PP
 .PP
-At least one of these flags must be set, failing which the API returns \fBCUDA_ERROR_INVALID_VALUE\fP. Both the flags are orthogonal to one another: a developer may set both these flags that allows to set both wait and signal specific attributes in the same \fCnvSciSyncAttrList\fP.
+At least one of these flags must be set, failing which the API returns \fBCUDA_ERROR_INVALID_VALUE\fP. Both the flags are orthogonal to one another: a developer may set both these flags that allows one to set both wait and signal specific attributes in the same \fCnvSciSyncAttrList\fP.
 .PP
 \fBParameters:\fP
 .RS 4
--- a/nvidia-cuda/doc/man/man3/cuda.h.3
+++ b/nvidia-cuda/doc/man/man3/cuda.h.3
@@ -557,7 +557,7 @@ cuda.h \- Header file for the CUDA Toolk
 .ti -1c
 .RI "\fBCUresult\fP \fBcuDeviceGetName\fP (char *name, int len, \fBCUdevice\fP dev)"
 .br
-.RI "\fIReturns an identifer string for the device. \fP"
+.RI "\fIReturns an identifier string for the device. \fP"
 .ti -1c
 .RI "\fBCUresult\fP \fBcuDeviceGetNvSciSyncAttributes\fP (void *nvSciSyncAttrList, \fBCUdevice\fP dev, int flags)"
 .br
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__TYPES.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__TYPES.html
@@ -2220,7 +2220,7 @@
                                  <dt><span class="enum-member-name-def">CU_JIT_FAST_COMPILE</span></dt>
                                  <dd></dd>
                                  <dt><span class="enum-member-name-def">CU_JIT_GLOBAL_SYMBOL_NAMES</span></dt>
-                                 <dd>Array of device symbol names that will be relocated to the corresponing host addresses stored in <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg5527fa8030d5cabedc781a04dbd1997d9ed137c8a50c7fac811ef7a63c5ae74f" shape="rect">CU_JIT_GLOBAL_SYMBOL_ADDRESSES</a>.
+                                 <dd>Array of device symbol names that will be relocated to the corresponding host addresses stored in <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg5527fa8030d5cabedc781a04dbd1997d9ed137c8a50c7fac811ef7a63c5ae74f" shape="rect">CU_JIT_GLOBAL_SYMBOL_ADDRESSES</a>.
                                     Must contain <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg5527fa8030d5cabedc781a04dbd1997dd6274c174e780689d0eeddd0a9ce1073" shape="rect">CU_JIT_GLOBAL_SYMBOL_COUNT</a> entries.
                                     When loding a device module, driver will relocate all encountered unresolved symbols to the host addresses.
                                     It is only allowed to register symbols that correspond to unresolved global variables.
@@ -2461,7 +2461,7 @@
                               </h6>
                               <dl class="enumerator">
                                  <dt><span class="enum-member-name-def">CU_MEM_ADVISE_SET_READ_MOSTLY = <span class="ph ph apiData">1</span></span></dt>
-                                 <dd>Data will mostly be read and only occassionally be written to </dd>
+                                 <dd>Data will mostly be read and only occasionally be written to </dd>
                                  <dt><span class="enum-member-name-def">CU_MEM_ADVISE_UNSET_READ_MOSTLY = <span class="ph ph apiData">2</span></span></dt>
                                  <dd>Undo the effect of <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1ggcfe2ed2d4567745dd4ad41034136fff35a99fb44378c84c56668550b94157fc0" shape="rect">CU_MEM_ADVISE_SET_READ_MOSTLY</a></dd>
                                  <dt><span class="enum-member-name-def">CU_MEM_ADVISE_SET_PREFERRED_LOCATION = <span class="ph ph apiData">3</span></span></dt>
--- a/nvidia-cuda/doc/man/man3/CUDA_TYPES.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_TYPES.3
@@ -1739,7 +1739,7 @@ Specifies whether to enable caching expl
 The below jit options are used for internal purposes only, in this version of CUDA 
 .TP
 \fB\fICU_JIT_GLOBAL_SYMBOL_NAMES \fP\fP
-Array of device symbol names that will be relocated to the corresponing host addresses stored in \fBCU_JIT_GLOBAL_SYMBOL_ADDRESSES\fP.
+Array of device symbol names that will be relocated to the corresponding host addresses stored in \fBCU_JIT_GLOBAL_SYMBOL_ADDRESSES\fP.
 .br
  Must contain \fBCU_JIT_GLOBAL_SYMBOL_COUNT\fP entries.
 .br
@@ -1882,7 +1882,7 @@ Memory advise values
 .in +1c
 .TP
 \fB\fICU_MEM_ADVISE_SET_READ_MOSTLY \fP\fP
-Data will mostly be read and only occassionally be written to 
+Data will mostly be read and only occasionally be written to 
 .TP
 \fB\fICU_MEM_ADVISE_UNSET_READ_MOSTLY \fP\fP
 Undo the effect of \fBCU_MEM_ADVISE_SET_READ_MOSTLY\fP 
@@ -1904,7 +1904,7 @@ Let the Unified Memory subsystem decide
 .in +1c
 .TP
 \fB\fICU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY \fP\fP
-Whether the range will mostly be read and only occassionally be written to 
+Whether the range will mostly be read and only occasionally be written to 
 .TP
 \fB\fICU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION \fP\fP
 The preferred location of the range 
@@ -2075,7 +2075,7 @@ Resource types
 .in +1c
 .TP
 \fB\fICU_RESOURCE_TYPE_ARRAY \fP\fP
-Array resoure 
+Array resource 
 .TP
 \fB\fICU_RESOURCE_TYPE_MIPMAPPED_ARRAY \fP\fP
 Mipmapped array resource 
--- a/nvidia-cuda/include/cuda.h
+++ b/nvidia-cuda/include/cuda.h
@@ -778,7 +778,7 @@ typedef enum CUcomputemode_enum {
  * Memory advise values
  */
 typedef enum CUmem_advise_enum {
-    CU_MEM_ADVISE_SET_READ_MOSTLY          = 1, /**< Data will mostly be read and only occassionally be written to */
+    CU_MEM_ADVISE_SET_READ_MOSTLY          = 1, /**< Data will mostly be read and only occasionally be written to */
     CU_MEM_ADVISE_UNSET_READ_MOSTLY        = 2, /**< Undo the effect of ::CU_MEM_ADVISE_SET_READ_MOSTLY */
     CU_MEM_ADVISE_SET_PREFERRED_LOCATION   = 3, /**< Set the preferred location for the data as the specified device */
     CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION = 4, /**< Clear the preferred location for the data */
@@ -787,7 +787,7 @@ typedef enum CUmem_advise_enum {
 } CUmem_advise;
 
 typedef enum CUmem_range_attribute_enum {
-    CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY            = 1, /**< Whether the range will mostly be read and only occassionally be written to */
+    CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY            = 1, /**< Whether the range will mostly be read and only occasionally be written to */
     CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION     = 2, /**< The preferred location of the range */
     CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY            = 3, /**< Memory range has ::CU_MEM_ADVISE_SET_ACCESSED_BY set for specified device */
     CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION = 4  /**< The last location to which the range was prefetched */
@@ -2845,7 +2845,7 @@ CUresult CUDAAPI cuDeviceGet(CUdevice *d
 CUresult CUDAAPI cuDeviceGetCount(int *count);
 
 /**
- * \brief Returns an identifer string for the device
+ * \brief Returns an identifier string for the device
  *
  * Returns an ASCII string identifying the device \p dev in the NULL-terminated
  * string pointed to by \p name. \p len specifies the maximum length of the
@@ -3131,7 +3131,7 @@ CUresult CUDAAPI cuDeviceTotalMem(size_t
  *   supports native atomic operations.
  * - ::CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: Ratio of single precision performance
  *   (in floating-point operations per second) to double precision performance.
- * - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS: Device suppports coherently accessing
+ * - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS: Device supports coherently accessing
  *   pageable memory without calling cudaHostRegister on it.
  * - ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS: Device can coherently access managed memory
  *   concurrently with the CPU.
@@ -3139,7 +3139,7 @@ CUresult CUDAAPI cuDeviceTotalMem(size_t
  * - ::CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: Device can access host registered
  *   memory at the same virtual address as the CPU.
  * -  ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: The maximum per block shared memory size
- *    suported on this device. This is the maximum value that can be opted into when using the cuFuncSetAttribute() call.
+ *    supported on this device. This is the maximum value that can be opted into when using the cuFuncSetAttribute() call.
  *    For more details see ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES
  * - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: Device accesses pageable memory via the host's
  *   page tables.
@@ -3196,7 +3196,7 @@ CUresult CUDAAPI cuDeviceGetAttribute(in
  *
  * At least one of these flags must be set, failing which the API
  * returns ::CUDA_ERROR_INVALID_VALUE. Both the flags are orthogonal
- * to one another: a developer may set both these flags that allows to
+ * to one another: a developer may set both these flags that allows one to
  * set both wait and signal specific attributes in the same \p nvSciSyncAttrList.
  *
  * \param nvSciSyncAttrList     - Return NvSciSync attributes supported.
@@ -3305,7 +3305,7 @@ __CUDA_DEPRECATED CUresult CUDAAPI cuDev
  *
  * \deprecated
  *
- * This function was deprecated as of CUDA 5.0 and its functionality superceded
+ * This function was deprecated as of CUDA 5.0 and its functionality superseded
  * by ::cuDeviceGetAttribute().
  *
  * Returns in \p *major and \p *minor the major and minor revision numbers that
@@ -3439,7 +3439,7 @@ CUresult CUDAAPI cuDevicePrimaryCtxRelea
 /**
  * \brief Set flags for the primary context
  *
- * Sets the flags for the primary context on the device overwriting perviously
+ * Sets the flags for the primary context on the device overwriting previously
  * set ones. If the primary context is already created
  * ::CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE is returned.
  *
@@ -5395,7 +5395,7 @@ CUresult CUDAAPI cuMemHostGetFlags(unsig
  * ::cuStreamAttachMemAsync will be required to enable access on such devices.
  *
  * If the association is later changed via ::cuStreamAttachMemAsync to
- * a single stream, the default association as specifed during ::cuMemAllocManaged
+ * a single stream, the default association as specified during ::cuMemAllocManaged
  * is restored when that stream is destroyed. For __managed__ variables, the
  * default association is always ::CU_MEM_ATTACH_GLOBAL. Note that destroying a
  * stream is an asynchronous operation, and as a result, the change to default
@@ -5733,7 +5733,7 @@ CUresult CUDAAPI cuIpcOpenMemHandle(CUde
 /**
  * \brief Close memory mapped with ::cuIpcOpenMemHandle
  *
- * Unmaps memory returnd by ::cuIpcOpenMemHandle. The original allocation
+ * Unmaps memory returned by ::cuIpcOpenMemHandle. The original allocation
  * in the exporting process as well as imported mappings in other processes
  * will be unaffected.
  *
@@ -8537,13 +8537,13 @@ CUresult CUDAAPI cuMemAddressFree(CUdevi
 * \brief Create a shareable memory handle representing a memory allocation of a given size described by the given properties
 *
 * This creates a memory allocation on the target device specified through the
-* \p prop strcuture. The created allocation will not have any device or host
+* \p prop structure. The created allocation will not have any device or host
 * mappings. The generic memory \p handle for the allocation can be
 * mapped to the address space of calling process via ::cuMemMap. This handle
 * cannot be transmitted directly to other processes (see
 * ::cuMemExportToShareableHandle).  On Windows, the caller must also pass
 * an LPSECURITYATTRIBUTE in \p prop to be associated with this handle which
-* limits or allows access to this handle for a recepient process (see
+* limits or allows access to this handle for a recipient process (see
 * ::CUmemAllocationProp::win32HandleMetaData for more).  The \p size of this
 * allocation must be a multiple of the the value given via
 * ::cuMemGetAllocationGranularity with the ::CU_MEM_ALLOC_GRANULARITY_MINIMUM
@@ -8577,7 +8577,7 @@ CUresult CUDAAPI cuMemCreate(CUmemGeneri
 * are unmapped and when all outstanding references to the handle (including it's
 * shareable counterparts) are also released. The generic memory handle can be
 * freed when there are still outstanding mappings made with this handle. Each
-* time a recepient process imports a shareable handle, it needs to pair it with
+* time a recipient process imports a shareable handle, it needs to pair it with
 * ::cuMemRelease for the handle to be freed.  If \p handle is not a valid handle
 * the behavior is undefined. 
 *
@@ -11669,7 +11669,7 @@ CUresult CUDAAPI cuLaunchCooperativeKern
  * All kernels launched must be identical with respect to the compiled code. Note that
  * any __device__, __constant__ or __managed__ variables present in the module that owns
  * the kernel launched on each device, are independently instantiated on every device.
- * It is the application's responsiblity to ensure these variables are initialized and
+ * It is the application's responsibility to ensure these variables are initialized and
  * used appropriately.
  *
  * The size of the grids as specified in blocks, the size of the blocks themselves
--- a/nvidia-cuda/include/cuda_runtime.h
+++ b/nvidia-cuda/include/cuda_runtime.h
@@ -171,8 +171,8 @@ struct  __device_builtin__ __nv_lambda_p
  * \p stream specifies a stream the invocation is associated to.
  *
  * \param func        - Device function symbol
- * \param gridDim     - Grid dimentions
- * \param blockDim    - Block dimentions
+ * \param gridDim     - Grid dimensions
+ * \param blockDim    - Block dimensions
  * \param args        - Arguments
  * \param sharedMem   - Shared memory (defaults to 0)
  * \param stream      - Stream identifier (defaults to NULL)
@@ -236,8 +236,8 @@ static __inline__ __host__ cudaError_t c
  * \p stream specifies a stream the invocation is associated to.
  *
  * \param func        - Device function symbol
- * \param gridDim     - Grid dimentions
- * \param blockDim    - Block dimentions
+ * \param gridDim     - Grid dimensions
+ * \param blockDim    - Block dimensions
  * \param args        - Arguments
  * \param sharedMem   - Shared memory (defaults to 0)
  * \param stream      - Stream identifier (defaults to NULL)
@@ -419,7 +419,7 @@ static __inline__ __host__ cudaError_t c
  * ::cudaStreamAttachMemAsync will be required to enable access on such devices.
  *
  * If the association is later changed via ::cudaStreamAttachMemAsync to
- * a single stream, the default association, as specifed during ::cudaMallocManaged,
+ * a single stream, the default association, as specified during ::cudaMallocManaged,
  * is restored when that stream is destroyed. For __managed__ variables, the
  * default association is always ::cudaMemAttachGlobal. Note that destroying a
  * stream is an asynchronous operation, and as a result, the change to default
@@ -1413,7 +1413,7 @@ static __inline__ __host__ cudaError_t c
  * streaming multiprocessor for the device function.
  *
  * \param numBlocks       - Returned occupancy
- * \param func            - Kernel function for which occupancy is calulated
+ * \param func            - Kernel function for which occupancy is calculated
  * \param blockSize       - Block size the kernel is intended to be launched with
  * \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
  *
@@ -1463,7 +1463,7 @@ static __inline__ __host__ cudaError_t c
  *   section of the Maxwell tuning guide.
  *
  * \param numBlocks       - Returned occupancy
- * \param func            - Kernel function for which occupancy is calulated
+ * \param func            - Kernel function for which occupancy is calculated
  * \param blockSize       - Block size the kernel is intended to be launched with
  * \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
  * \param flags           - Requested behavior for the occupancy calculator
@@ -1805,7 +1805,7 @@ static __inline__ __host__ CUDART_DEVICE
 }
 
 /**
- * \brief Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags
+ * \brief Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags
  *
  * Returns in \p *minGridSize and \p *blocksize a suggested grid /
  * block size pair that achieves the best potential occupancy
--- a/nvidia-cuda/include/cuda_runtime_api.h
+++ b/nvidia-cuda/include/cuda_runtime_api.h
@@ -396,7 +396,7 @@ extern __host__ cudaError_t CUDARTAPI cu
  * - ::cudaLimitMallocHeapSize: size in bytes of the heap used by the
  *   ::malloc() and ::free() device system calls;
  * - ::cudaLimitDevRuntimeSyncDepth: maximum grid depth at which a
- *   thread can isssue the device runtime call ::cudaDeviceSynchronize()
+ *   thread can issue the device runtime call ::cudaDeviceSynchronize()
  *   to wait on child grid launches to complete.
  * - ::cudaLimitDevRuntimePendingLaunchCount: maximum number of outstanding
  *   device runtime launches.
@@ -856,7 +856,7 @@ extern __host__ cudaError_t CUDARTAPI cu
 /**
  * \brief Close memory mapped with cudaIpcOpenMemHandle
  *
- * Unmaps memory returnd by ::cudaIpcOpenMemHandle. The original allocation
+ * Unmaps memory returned by ::cudaIpcOpenMemHandle. The original allocation
  * in the exporting process as well as imported mappings in other processes
  * will be unaffected.
  *
@@ -1771,7 +1771,7 @@ extern __host__ __cudart_builtin__ cudaE
  *
  * At least one of these flags must be set, failing which the API
  * returns ::cudaErrorInvalidValue. Both the flags are orthogonal
- * to one another: a developer may set both these flags that allows to
+ * to one another: a developer may set both these flags that allows one to
  * set both wait and signal specific attributes in the same \p nvSciSyncAttrList.
  *
  * \param nvSciSyncAttrList     - Return NvSciSync attributes supported.
@@ -3562,8 +3562,8 @@ extern __host__ cudaError_t CUDARTAPI cu
  * \p stream specifies a stream the invocation is associated to.
  *
  * \param func        - Device function symbol
- * \param gridDim     - Grid dimentions
- * \param blockDim    - Block dimentions
+ * \param gridDim     - Grid dimensions
+ * \param blockDim    - Block dimensions
  * \param args        - Arguments
  * \param sharedMem   - Shared memory
  * \param stream      - Stream identifier
@@ -3620,8 +3620,8 @@ extern __host__ cudaError_t CUDARTAPI cu
  * \p stream specifies a stream the invocation is associated to.
  *
  * \param func        - Device function symbol
- * \param gridDim     - Grid dimentions
- * \param blockDim    - Block dimentions
+ * \param gridDim     - Grid dimensions
+ * \param blockDim    - Block dimensions
  * \param args        - Arguments
  * \param sharedMem   - Shared memory
  * \param stream      - Stream identifier
@@ -3661,7 +3661,7 @@ extern __host__ cudaError_t CUDARTAPI cu
  *
  * The same kernel must be launched on all devices. Note that any __device__ or __constant__
  * variables are independently instantiated on every device. It is the application's
- * responsiblity to ensure these variables are initialized and used appropriately.
+ * responsibility to ensure these variables are initialized and used appropriately.
  *
  * The size of the grids as specified in blocks, the size of the blocks themselves and the
  * amount of shared memory used by each thread block must also match across all launched kernels.
@@ -4177,7 +4177,7 @@ extern __host__ __cudart_builtin__ cudaE
  * ::cudaStreamAttachMemAsync will be required to enable access on such devices.
  *
  * If the association is later changed via ::cudaStreamAttachMemAsync to
- * a single stream, the default association, as specifed during ::cudaMallocManaged,
+ * a single stream, the default association, as specified during ::cudaMallocManaged,
  * is restored when that stream is destroyed. For __managed__ variables, the
  * default association is always ::cudaMemAttachGlobal. Note that destroying a
  * stream is an asynchronous operation, and as a result, the change to default
@@ -5124,7 +5124,7 @@ struct cudaMemcpy3DParms {
 };
 \endcode
  *
- * ::cudaMemcpy3D() copies data betwen two 3D objects. The source and
+ * ::cudaMemcpy3D() copies data between two 3D objects. The source and
  * destination objects may be in either host memory, device memory, or a CUDA
  * array. The source, destination, extent, and kind of copy performed is
  * specified by the ::cudaMemcpy3DParms struct which should be initialized to
@@ -5260,7 +5260,7 @@ struct cudaMemcpy3DParms {
 };
 \endcode
  *
- * ::cudaMemcpy3DAsync() copies data betwen two 3D objects. The source and
+ * ::cudaMemcpy3DAsync() copies data between two 3D objects. The source and
  * destination objects may be in either host memory, device memory, or a CUDA
  * array. The source, destination, extent, and kind of copy performed is
  * specified by the ::cudaMemcpy3DParms struct which should be initialized to
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__MEM.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__MEM.html
@@ -1004,7 +1004,7 @@
                            </div>
                            <div class="section">
                               <h6 class="description_header">Description</h6>
-                              <p>Unmaps memory returnd by <a class="xref" href="group__CUDA__MEM.html#group__CUDA__MEM_1ga8bd126fcff919a0c996b7640f197b79" title="Opens an interprocess memory handle exported from another process and returns a device pointer usable in the local process." shape="rect">cuIpcOpenMemHandle</a>. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
+                              <p>Unmaps memory returned by <a class="xref" href="group__CUDA__MEM.html#group__CUDA__MEM_1ga8bd126fcff919a0c996b7640f197b79" title="Opens an interprocess memory handle exported from another process and returns a device pointer usable in the local process." shape="rect">cuIpcOpenMemHandle</a>. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
                               </p>
                               <p class="p">Any resources used to enable peer access will be freed if this is the last mapping using them.</p>
                               <p class="p">IPC functionality is restricted to devices with support for unified addressing on Linux and Windows operating systems. IPC
@@ -1291,7 +1291,7 @@
                               </p>
                               <p class="p"><tt class="ph tt code">flags</tt> specifies the default stream association for this allocation. <tt class="ph tt code">flags</tt> must be one of <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg17c5d5f9b585aa2d6f121847d1a78f4c0b42aae6a29b41b734d4c0dea6c33313" shape="rect">CU_MEM_ATTACH_GLOBAL</a> or <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg17c5d5f9b585aa2d6f121847d1a78f4c8b59c62cab9c7a762338e5fae92e2e9c" shape="rect">CU_MEM_ATTACH_HOST</a>. If <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg17c5d5f9b585aa2d6f121847d1a78f4c0b42aae6a29b41b734d4c0dea6c33313" shape="rect">CU_MEM_ATTACH_GLOBAL</a> is specified, then this memory is accessible from any stream on any device. If <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg17c5d5f9b585aa2d6f121847d1a78f4c8b59c62cab9c7a762338e5fae92e2e9c" shape="rect">CU_MEM_ATTACH_HOST</a> is specified, then the allocation should not be accessed from devices that have a zero value for the device attribute <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a333110e44c9cb6ead02f03ff6f6fd495e" shape="rect">CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS</a>; an explicit call to <a class="xref" href="group__CUDA__STREAM.html#group__CUDA__STREAM_1g6e468d680e263e7eba02a56643c50533" title="Attach memory to a stream asynchronously." shape="rect">cuStreamAttachMemAsync</a> will be required to enable access on such devices.
                               </p>
-                              <p class="p">If the association is later changed via <a class="xref" href="group__CUDA__STREAM.html#group__CUDA__STREAM_1g6e468d680e263e7eba02a56643c50533" title="Attach memory to a stream asynchronously." shape="rect">cuStreamAttachMemAsync</a> to a single stream, the default association as specifed during <a class="xref" href="group__CUDA__MEM.html#group__CUDA__MEM_1gb347ded34dc326af404aa02af5388a32" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cuMemAllocManaged</a> is restored when that stream is destroyed. For __managed__ variables, the default association is always <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg17c5d5f9b585aa2d6f121847d1a78f4c0b42aae6a29b41b734d4c0dea6c33313" shape="rect">CU_MEM_ATTACH_GLOBAL</a>. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen
+                              <p class="p">If the association is later changed via <a class="xref" href="group__CUDA__STREAM.html#group__CUDA__STREAM_1g6e468d680e263e7eba02a56643c50533" title="Attach memory to a stream asynchronously." shape="rect">cuStreamAttachMemAsync</a> to a single stream, the default association as specified during <a class="xref" href="group__CUDA__MEM.html#group__CUDA__MEM_1gb347ded34dc326af404aa02af5388a32" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cuMemAllocManaged</a> is restored when that stream is destroyed. For __managed__ variables, the default association is always <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg17c5d5f9b585aa2d6f121847d1a78f4c0b42aae6a29b41b734d4c0dea6c33313" shape="rect">CU_MEM_ATTACH_GLOBAL</a>. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen
                                  until all work in the stream has completed.
                               </p>
                               <p class="p">Memory allocated with <a class="xref" href="group__CUDA__MEM.html#group__CUDA__MEM_1gb347ded34dc326af404aa02af5388a32" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cuMemAllocManaged</a> should be released with <a class="xref" href="group__CUDA__MEM.html#group__CUDA__MEM_1g89b3f154e17cc89b6eea277dbdf5c93a" title="Frees device memory." shape="rect">cuMemFree</a>.
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__DEVICE.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__DEVICE.html
@@ -1058,7 +1058,7 @@
                                        </p>
                                     </li>
                                     <li class="li">
-                                       <p class="p"><a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg4c4b34c054d383b0e9a63ab0ffc9365123c4900be4af436cb769e4c72d07be11" shape="rect">cudaLimitDevRuntimeSyncDepth</a>: maximum grid depth at which a thread can isssue the device runtime call <a class="xref" href="group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d" title="Wait for compute device to finish." shape="rect">cudaDeviceSynchronize()</a> to wait on child grid launches to complete.
+                                       <p class="p"><a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg4c4b34c054d383b0e9a63ab0ffc9365123c4900be4af436cb769e4c72d07be11" shape="rect">cudaLimitDevRuntimeSyncDepth</a>: maximum grid depth at which a thread can issue the device runtime call <a class="xref" href="group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d" title="Wait for compute device to finish." shape="rect">cudaDeviceSynchronize()</a> to wait on child grid launches to complete.
                                        </p>
                                     </li>
                                     <li class="li">
@@ -1134,7 +1134,7 @@
                                     </li>
                                  </ul>
                               </p>
-                              <p class="p">At least one of these flags must be set, failing which the API returns <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg3f51e3575c2178246db0a94a430e00383e8aef5398ee38e28ed41e357b48917c" shape="rect">cudaErrorInvalidValue</a>. Both the flags are orthogonal to one another: a developer may set both these flags that allows to set both wait and signal
+                              <p class="p">At least one of these flags must be set, failing which the API returns <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg3f51e3575c2178246db0a94a430e00383e8aef5398ee38e28ed41e357b48917c" shape="rect">cudaErrorInvalidValue</a>. Both the flags are orthogonal to one another: a developer may set both these flags that allows one to set both wait and signal
                                  specific attributes in the same <tt class="ph tt code">nvSciSyncAttrList</tt>.
                               </p>
                               <p class="p"></p>
@@ -2330,7 +2330,7 @@
                            </div>
                            <div class="section">
                               <h6 class="description_header">Description</h6>
-                              <p>Unmaps memory returnd by <a class="xref" href="group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g01050a29fefde385b1042081ada4cde9" title="Opens an interprocess memory handle exported from another process and returns a device pointer usable in the local process." shape="rect">cudaIpcOpenMemHandle</a>. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
+                              <p>Unmaps memory returned by <a class="xref" href="group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g01050a29fefde385b1042081ada4cde9" title="Opens an interprocess memory handle exported from another process and returns a device pointer usable in the local process." shape="rect">cudaIpcOpenMemHandle</a>. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
                               </p>
                               <p class="p">Any resources used to enable peer access will be freed if this is the last mapping using them.</p>
                               <p class="p">IPC functionality is restricted to devices with support for unified addressing on Linux operating systems. IPC functionality
--- a/nvidia-cuda/doc/man/man3/CUDART_DEVICE.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_DEVICE.3
@@ -448,7 +448,7 @@ Returns in \fC*pValue\fP the current siz
 .IP "\(bu" 2
 \fBcudaLimitMallocHeapSize\fP: size in bytes of the heap used by the malloc() and free() device system calls;
 .IP "\(bu" 2
-\fBcudaLimitDevRuntimeSyncDepth\fP: maximum grid depth at which a thread can isssue the device runtime call \fBcudaDeviceSynchronize()\fP to wait on child grid launches to complete.
+\fBcudaLimitDevRuntimeSyncDepth\fP: maximum grid depth at which a thread can issue the device runtime call \fBcudaDeviceSynchronize()\fP to wait on child grid launches to complete.
 .IP "\(bu" 2
 \fBcudaLimitDevRuntimePendingLaunchCount\fP: maximum number of outstanding device runtime launches.
 .IP "\(bu" 2
@@ -493,7 +493,7 @@ The \fCflags\fP controls how application
 \fBcudaNvSciSyncAttrWait\fP, specifies that the applications intends to wait on an NvSciSync on this CUDA device.
 .PP
 .PP
-At least one of these flags must be set, failing which the API returns \fBcudaErrorInvalidValue\fP. Both the flags are orthogonal to one another: a developer may set both these flags that allows to set both wait and signal specific attributes in the same \fCnvSciSyncAttrList\fP.
+At least one of these flags must be set, failing which the API returns \fBcudaErrorInvalidValue\fP. Both the flags are orthogonal to one another: a developer may set both these flags that allows one to set both wait and signal specific attributes in the same \fCnvSciSyncAttrList\fP.
 .PP
 \fBParameters:\fP
 .RS 4
@@ -1170,7 +1170,7 @@ Note that this function may also return
 
 .SS "\fBcudaError_t\fP cudaIpcCloseMemHandle (void * devPtr)"
 .PP
-Unmaps memory returnd by \fBcudaIpcOpenMemHandle\fP. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
+Unmaps memory returned by \fBcudaIpcOpenMemHandle\fP. The original allocation in the exporting process as well as imported mappings in other processes will be unaffected.
 .PP
 Any resources used to enable peer access will be freed if this is the last mapping using them.
 .PP
--- a/nvidia-cuda/doc/html/cublas/index.html
+++ b/nvidia-cuda/doc/html/cublas/index.html
@@ -37981,7 +37981,7 @@ cublasXtDestroy(cublasXtHandle_t handle)
                               When enabled, the matrices passed in subsequent  cublasXt API calls will be pinned/unpinned using the CUDART routine <samp class="ph codeph">cudaHostRegister</samp> and 
                               <samp class="ph codeph">cudaHostUnregister</samp> respectively if the matrices are not already pinned. 
                               If a matrix happened to be pinned partially, it will also not be pinned.
-                              Pinning the memory improve PCI transfer performace and allows to overlap PCI memory transfer with computation. However pinning/unpinning
+                              Pinning the memory improve PCI transfer performace and allows one to overlap PCI memory transfer with computation. However pinning/unpinning
                               the memory take some time which might not be amortized.
                               It is advised that the user pins the memory on its own using <samp class="ph codeph">cudaMallocHost</samp> or <samp class="ph codeph">cudaHostRegister</samp> and unpin it when the computation sequence is completed.
                               By default, the Pinning Memory mode is disabled.
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__DEVICE.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__DEVICE.html
@@ -326,7 +326,7 @@
                      <dt><span class="member_type"><a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gc6c391505e117393cc2558fff6bfc2e9" title="" shape="rect">CUresult</a>&nbsp;</span><span class="member_name"><a href="#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" shape="rect">cuDeviceGetLuid</a> (  char*<span>&nbsp;</span><span class="keyword keyword apiItemName">luid</span>, unsigned int*<span>&nbsp;</span><span class="keyword keyword apiItemName">deviceNodeMask</span>, <a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gcd81b70eb9968392bb5cdf582af8eab4" title="" shape="rect">CUdevice</a><span>&nbsp;</span><span class="keyword keyword apiItemName">dev</span> ) </span></dt>
                      <dd class="shortdesc"><span></span><span class="desc">Return an LUID and device node mask for the device. </span></dd>
                      <dt><span class="member_type"><a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gc6c391505e117393cc2558fff6bfc2e9" title="" shape="rect">CUresult</a>&nbsp;</span><span class="member_name"><a href="#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" shape="rect">cuDeviceGetName</a> (  char*<span>&nbsp;</span><span class="keyword keyword apiItemName">name</span>, int <span>&nbsp;</span><span class="keyword keyword apiItemName">len</span>, <a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gcd81b70eb9968392bb5cdf582af8eab4" title="" shape="rect">CUdevice</a><span>&nbsp;</span><span class="keyword keyword apiItemName">dev</span> ) </span></dt>
-                     <dd class="shortdesc"><span></span><span class="desc">Returns an identifer string for the device. </span></dd>
+                     <dd class="shortdesc"><span></span><span class="desc">Returns an identifier string for the device. </span></dd>
                      <dt><span class="member_type"><a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gc6c391505e117393cc2558fff6bfc2e9" title="" shape="rect">CUresult</a>&nbsp;</span><span class="member_name"><a href="#group__CUDA__DEVICE_1g0991e2b2b3cedee1ca77d6376e581335" shape="rect">cuDeviceGetNvSciSyncAttributes</a> (  void*<span>&nbsp;</span><span class="keyword keyword apiItemName">nvSciSyncAttrList</span>, <a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gcd81b70eb9968392bb5cdf582af8eab4" title="" shape="rect">CUdevice</a><span>&nbsp;</span><span class="keyword keyword apiItemName">dev</span>, int <span>&nbsp;</span><span class="keyword keyword apiItemName">flags</span> ) </span></dt>
                      <dd class="shortdesc"><span></span><span class="desc">Return NvSciSync attributes that this device can support. </span></dd>
                      <dt><span class="member_type"><a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gc6c391505e117393cc2558fff6bfc2e9" title="" shape="rect">CUresult</a>&nbsp;</span><span class="member_name"><a href="#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" shape="rect">cuDeviceGetUuid</a> (  CUuuid*<span>&nbsp;</span><span class="keyword keyword apiItemName">uuid</span>, <a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gcd81b70eb9968392bb5cdf582af8eab4" title="" shape="rect">CUdevice</a><span>&nbsp;</span><span class="keyword keyword apiItemName">dev</span> ) </span></dt>
@@ -369,7 +369,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" title="Return an LUID and device node mask for the device." shape="rect">cuDeviceGetLuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" title="Return an LUID and device node mask for the device." shape="rect">cuDeviceGetLuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -743,7 +743,7 @@
                                        </p>
                                     </li>
                                     <li class="li">
-                                       <p class="p"><a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a35fdcdbe1dfc3ad5ec428c279e0efb9cd" shape="rect">CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS</a>: Device suppports coherently accessing pageable memory without calling cudaHostRegister on it.
+                                       <p class="p"><a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a35fdcdbe1dfc3ad5ec428c279e0efb9cd" shape="rect">CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS</a>: Device supports coherently accessing pageable memory without calling cudaHostRegister on it.
                                        </p>
                                     </li>
                                     <li class="li">
@@ -759,7 +759,7 @@
                                        </p>
                                     </li>
                                     <li class="li">
-                                       <p class="p"><a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a3e788564c0a95b866dc624fbc1b49dab3" shape="rect">CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN</a>: The maximum per block shared memory size suported on this device. This is the maximum value that can be opted into when
+                                       <p class="p"><a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gge12b8a782bebe21b1ac0091bf9f4e2a3e788564c0a95b866dc624fbc1b49dab3" shape="rect">CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN</a>: The maximum per block shared memory size supported on this device. This is the maximum value that can be opted into when
                                           using the <a class="xref" href="group__CUDA__EXEC.html#group__CUDA__EXEC_1g0e37dce0173bc883aa1e5b14dd747f26" title="Sets information about a function." shape="rect">cuFuncSetAttribute()</a> call. For more details see <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg9d955dde0904a9b43ca4d875ac1551bc75b33d145e83462ef7292575015be03e" shape="rect">CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES</a></p>
                                     </li>
                                     <li class="li">
@@ -793,7 +793,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gb22e8256592b836df9a9cc36c9db7151" target="_blank" shape="rect">cudaDeviceGetAttribute</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0" target="_blank" shape="rect">cudaGetDeviceProperties</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gb22e8256592b836df9a9cc36c9db7151" target="_blank" shape="rect">cudaDeviceGetAttribute</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0" target="_blank" shape="rect">cudaGetDeviceProperties</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -828,7 +828,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" title="Return an LUID and device node mask for the device." shape="rect">cuDeviceGetLuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f" target="_blank" shape="rect">cudaGetDeviceCount</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" title="Return an LUID and device node mask for the device." shape="rect">cuDeviceGetLuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f" target="_blank" shape="rect">cudaGetDeviceCount</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -866,14 +866,14 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0" target="_blank" shape="rect">cudaGetDeviceProperties</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0" target="_blank" shape="rect">cudaGetDeviceProperties</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
                         <dt class="description"><a name="group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" id="group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" shape="rect">
                               <!-- --></a><span><a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gc6c391505e117393cc2558fff6bfc2e9" title="" shape="rect">CUresult</a> cuDeviceGetName (  char*<span>&nbsp;</span><span class="keyword keyword apiItemName">name</span>, int <span>&nbsp;</span><span class="keyword keyword apiItemName">len</span>, <a href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gcd81b70eb9968392bb5cdf582af8eab4" title="" shape="rect">CUdevice</a><span>&nbsp;</span><span class="keyword keyword apiItemName">dev</span> ) </span></dt>
                         <dd class="description">
-                           <div class="section">Returns an identifer string for the device. </div>
+                           <div class="section">Returns an identifier string for the device. </div>
                            <div class="section">
                               <h6 class="parameter_header">
                                  Parameters
@@ -946,7 +946,7 @@
                                     </li>
                                  </ul>
                               </p>
-                              <p class="p">At least one of these flags must be set, failing which the API returns <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1ggc6c391505e117393cc2558fff6bfc2e990696c86fcee1f536a1ec7d25867feeb" shape="rect">CUDA_ERROR_INVALID_VALUE</a>. Both the flags are orthogonal to one another: a developer may set both these flags that allows to set both wait and signal
+                              <p class="p">At least one of these flags must be set, failing which the API returns <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1ggc6c391505e117393cc2558fff6bfc2e990696c86fcee1f536a1ec7d25867feeb" shape="rect">CUDA_ERROR_INVALID_VALUE</a>. Both the flags are orthogonal to one another: a developer may set both these flags that allows one to set both wait and signal
                                  specific attributes in the same <tt class="ph tt code">nvSciSyncAttrList</tt>.
                               </p>
                               <p class="p"></p>
@@ -988,7 +988,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" title="Return an LUID and device node mask for the device." shape="rect">cuDeviceGetLuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0" target="_blank" shape="rect">cudaGetDeviceProperties</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g630073c868f8878e89705ea831c49249" title="Return an LUID and device node mask for the device." shape="rect">cuDeviceGetLuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0" target="_blank" shape="rect">cudaGetDeviceProperties</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -1024,7 +1024,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g376b97f5ab20321ca46f7cfa9511b978" target="_blank" shape="rect">cudaMemGetInfo</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="../cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g376b97f5ab20321ca46f7cfa9511b978" target="_blank" shape="rect">cudaMemGetInfo</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__DEVICE__DEPRECATED.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__DEVICE__DEPRECATED.html
@@ -349,7 +349,7 @@
                            </div>
                            <div class="section">
                               <h6 class="deprecated_header"><a class="xref xrefsect-title" href="deprecated.html#deprecated__deprecated_1_deprecated000002" shape="rect">Deprecated</a></h6>
-                              <p>This function was deprecated as of CUDA 5.0 and its functionality superceded by <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute()</a>.
+                              <p>This function was deprecated as of CUDA 5.0 and its functionality superseded by <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute()</a>.
                               </p>
                               <h6 class="description_header">Description</h6>
                               <p class="p">Returns in <tt class="ph tt code">*major</tt> and <tt class="ph tt code">*minor</tt> the major and minor revision numbers that define the compute capability of the device <tt class="ph tt code">dev</tt>.
@@ -362,7 +362,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -450,7 +450,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifer string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g9c3e1414f0ad901d3278a4d6645fc266" title="Returns information about the device." shape="rect">cuDeviceGetAttribute</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g52b5ce05cb8c5fb6831b2c0ff2887c74" title="Returns the number of compute-capable devices." shape="rect">cuDeviceGetCount</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gef75aa30df95446a845f2a7b9fffbb7f" title="Returns an identifier string for the device." shape="rect">cuDeviceGetName</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g987b46b884c101ed5be414ab4d9e60e4" title="Return an UUID for the device." shape="rect">cuDeviceGetUuid</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1g8bdd1cc7201304b01357b8034f6587cb" title="Returns a handle to a compute device." shape="rect">cuDeviceGet</a>, <a class="xref" href="group__CUDA__DEVICE.html#group__CUDA__DEVICE_1gc6a0d6551335a3780f9f3c967a0fde5d" title="Returns the total amount of memory on the device." shape="rect">cuDeviceTotalMem</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__EGL.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__EGL.html
@@ -464,7 +464,7 @@
                               </h6>
                               <dl class="table-display-params">
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">conn</span></tt></dt>
-                                 <dd>- Conection to disconnect.</dd>
+                                 <dd>- Connection to disconnect.</dd>
                               </dl>
                            </div>
                            <div class="section">
@@ -566,7 +566,7 @@
                               </h6>
                               <dl class="table-display-params">
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">conn</span></tt></dt>
-                                 <dd>- Conection to disconnect.</dd>
+                                 <dd>- Connection to disconnect.</dd>
                               </dl>
                            </div>
                            <div class="section">
@@ -750,7 +750,7 @@
                                  operation which access the objects have completed before passing control to CUDA. This may be accomplished by issuing and
                                  waiting for glFinish command on all GLcontexts (for OpenGL and likewise for other APIs). The application will be also responsible
                                  for ensuring that any pending operation on the registered CUDA resource has completed prior to executing subsequent commands
-                                 in other APIs accesing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize
+                                 in other APIs accessing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize
                                  (preferably).
                               </p>
                               <p class="p">The surface's intended usage is specified using <tt class="ph tt code">flags</tt>, as follows:
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__EXEC.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__EXEC.html
@@ -711,7 +711,7 @@
                               </p>
                               <p class="p">All kernels launched must be identical with respect to the compiled code. Note that any __device__, __constant__ or __managed__
                                  variables present in the module that owns the kernel launched on each device, are independently instantiated on every device.
-                                 It is the application's responsiblity to ensure these variables are initialized and used appropriately.
+                                 It is the application's responsibility to ensure these variables are initialized and used appropriately.
                               </p>
                               <p class="p">The size of the grids as specified in blocks, the size of the blocks themselves and the amount of shared memory used by each
                                  thread block must also match across all launched kernels.
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__PRIMARY__CTX.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__PRIMARY__CTX.html
@@ -509,7 +509,7 @@
                            </div>
                            <div class="section">
                               <h6 class="description_header">Description</h6>
-                              <p>Sets the flags for the primary context on the device overwriting perviously set ones. If the primary context is already created
+                              <p>Sets the flags for the primary context on the device overwriting previously set ones. If the primary context is already created
                                  <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1ggc6c391505e117393cc2558fff6bfc2e90d74711f16f63a410d8d94d02e1f5480" shape="rect">CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE</a> is returned.
                               </p>
                               <p class="p">The three LSBs of the <tt class="ph tt code">flags</tt> parameter can be used to control how the OS thread, which owns the CUDA context at the time of an API call, interacts with
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__EGL.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__EGL.html
@@ -463,7 +463,7 @@
                               </h6>
                               <dl class="table-display-params">
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">conn</span></tt></dt>
-                                 <dd>- Conection to disconnect.</dd>
+                                 <dd>- Connection to disconnect.</dd>
                               </dl>
                            </div>
                            <div class="section">
@@ -563,7 +563,7 @@
                               </h6>
                               <dl class="table-display-params">
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">conn</span></tt></dt>
-                                 <dd>- Conection to disconnect.</dd>
+                                 <dd>- Connection to disconnect.</dd>
                               </dl>
                            </div>
                            <div class="section">
@@ -737,7 +737,7 @@
                                  operation which access the objects have completed before passing control to CUDA. This may be accomplished by issuing and
                                  waiting for glFinish command on all GLcontexts (for OpenGL and likewise for other APIs). The application will be also responsible
                                  for ensuring that any pending operation on the registered CUDA resource has completed prior to executing subsequent commands
-                                 in other APIs accesing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize
+                                 in other APIs accessing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize
                                  (preferably).
                               </p>
                               <p class="p">The surface's intended usage is specified using <tt class="ph tt code">flags</tt>, as follows:
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__EXECUTION.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__EXECUTION.html
@@ -727,9 +727,9 @@
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">func</span></tt></dt>
                                  <dd>- Device function symbol </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">gridDim</span></tt></dt>
-                                 <dd>- Grid dimentions </dd>
+                                 <dd>- Grid dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">blockDim</span></tt></dt>
-                                 <dd>- Block dimentions </dd>
+                                 <dd>- Block dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">args</span></tt></dt>
                                  <dd>- Arguments </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">sharedMem</span></tt></dt>
@@ -819,7 +819,7 @@
                                  All devices must have a non-zero value for the device attribute <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg49e2f8c2c0bd6fe264f2fc970912e5cdd23f91b7405936dcbdb32cccf4598ea3" shape="rect">cudaDevAttrCooperativeMultiDeviceLaunch</a>.
                               </p>
                               <p class="p">The same kernel must be launched on all devices. Note that any __device__ or __constant__ variables are independently instantiated
-                                 on every device. It is the application's responsiblity to ensure these variables are initialized and used appropriately.
+                                 on every device. It is the application's responsibility to ensure these variables are initialized and used appropriately.
                               </p>
                               <p class="p">The size of the grids as specified in blocks, the size of the blocks themselves and the amount of shared memory used by each
                                  thread block must also match across all launched kernels.
@@ -963,7 +963,7 @@
                                     </li>
                                  </ul>
                               </p>
-                              <p class="p">Note that, in constrast to <a class="xref" href="../cuda-driver-api/group__CUDA__STREAM.html#group__CUDA__STREAM_1g613d97a277d7640f4cb1c03bd51c2483" target="_blank" shape="rect">cuStreamAddCallback</a>, the function will not be called in the event of an error in the CUDA context.
+                              <p class="p">Note that, in contrast to <a class="xref" href="../cuda-driver-api/group__CUDA__STREAM.html#group__CUDA__STREAM_1g613d97a277d7640f4cb1c03bd51c2483" target="_blank" shape="rect">cuStreamAddCallback</a>, the function will not be called in the event of an error in the CUDA context.
                               </p>
                               <p class="p"></p>
                               <p class="p"></p>
@@ -1007,9 +1007,9 @@
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">func</span></tt></dt>
                                  <dd>- Device function symbol </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">gridDim</span></tt></dt>
-                                 <dd>- Grid dimentions </dd>
+                                 <dd>- Grid dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">blockDim</span></tt></dt>
-                                 <dd>- Block dimentions </dd>
+                                 <dd>- Block dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">args</span></tt></dt>
                                  <dd>- Arguments </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">sharedMem</span></tt></dt>
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__HIGHLEVEL.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__HIGHLEVEL.html
@@ -406,7 +406,7 @@
                      <dd class="shortdesc"><span></span><span class="desc">Returns grid and block size that achieves maximum potential occupancy for a device function. </span></dd>
                      <dt><span class="template">template &lt; class T &gt;</span><span class="member_type"><span class="keyword keyword apiItemName">__host__</span>
                            ​<a href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6" title="" shape="rect">cudaError_t</a>&nbsp;</span><span class="member_name"><a href="#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a> (  int*<span>&nbsp;</span><span class="keyword keyword apiItemName">minGridSize</span>, int*<span>&nbsp;</span><span class="keyword keyword apiItemName">blockSize</span>, T<span>&nbsp;</span><span class="keyword keyword apiItemName">func</span>, size_t<span>&nbsp;</span><span class="keyword keyword apiItemName">dynamicSMemSize</span> = <span class="ph ph apiData">0</span>, int <span>&nbsp;</span><span class="keyword keyword apiItemName">blockSizeLimit</span> = <span class="ph ph apiData">0</span>, unsigned int <span>&nbsp;</span><span class="keyword keyword apiItemName">flags</span> = <span class="ph ph apiData">0</span> ) </span></dt>
-                     <dd class="shortdesc"><span></span><span class="desc">Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags. </span></dd>
+                     <dd class="shortdesc"><span></span><span class="desc">Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags. </span></dd>
                      <dt><span class="template">template &lt; class T &gt;</span><span class="member_type"><span class="keyword keyword apiItemName">__host__</span>
                            ​<a href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6" title="" shape="rect">cudaError_t</a>&nbsp;</span><span class="member_name"><a href="#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" shape="rect">cudaStreamAttachMemAsync</a> (  <a href="group__CUDART__TYPES.html#group__CUDART__TYPES_1ge15d9c8b7a240312b533d6122558085a" title="" shape="rect">cudaStream_t</a><span>&nbsp;</span><span class="keyword keyword apiItemName">stream</span>, T*<span>&nbsp;</span><span class="keyword keyword apiItemName">devPtr</span>, size_t<span>&nbsp;</span><span class="keyword keyword apiItemName">length</span> = <span class="ph ph apiData">0</span>, unsigned int <span>&nbsp;</span><span class="keyword keyword apiItemName">flags</span> = <span class="ph ph apiData">cudaMemAttachSingle</span> ) </span></dt>
                      <dd class="shortdesc"><span></span><span class="desc">Attach memory to a stream asynchronously. </span></dd>
@@ -1377,9 +1377,9 @@
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">func</span></tt></dt>
                                  <dd>- Device function symbol </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">gridDim</span></tt></dt>
-                                 <dd>- Grid dimentions </dd>
+                                 <dd>- Grid dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">blockDim</span></tt></dt>
-                                 <dd>- Block dimentions </dd>
+                                 <dd>- Block dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">args</span></tt></dt>
                                  <dd>- Arguments </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">sharedMem</span></tt></dt>
@@ -1450,9 +1450,9 @@
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">func</span></tt></dt>
                                  <dd>- Device function symbol </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">gridDim</span></tt></dt>
-                                 <dd>- Grid dimentions </dd>
+                                 <dd>- Grid dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">blockDim</span></tt></dt>
-                                 <dd>- Block dimentions </dd>
+                                 <dd>- Block dimensions </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">args</span></tt></dt>
                                  <dd>- Arguments </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">sharedMem</span></tt></dt>
@@ -1622,7 +1622,7 @@
                               </p>
                               <p class="p"><tt class="ph tt code">flags</tt> specifies the default stream association for this allocation. <tt class="ph tt code">flags</tt> must be one of <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a> or <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4f9a428d18fdd89a99441d0dd27131c0" shape="rect">cudaMemAttachHost</a>. The default value for <tt class="ph tt code">flags</tt> is <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a>. If <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a> is specified, then this memory is accessible from any stream on any device. If <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4f9a428d18fdd89a99441d0dd27131c0" shape="rect">cudaMemAttachHost</a> is specified, then the allocation should not be accessed from devices that have a zero value for the device attribute <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg49e2f8c2c0bd6fe264f2fc970912e5cdc88178f29891f2c18fe67361cc80de09" shape="rect">cudaDevAttrConcurrentManagedAccess</a>; an explicit call to <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" title="Attach memory to a stream asynchronously." shape="rect">cudaStreamAttachMemAsync</a> will be required to enable access on such devices.
                               </p>
-                              <p class="p">If the association is later changed via <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" title="Attach memory to a stream asynchronously." shape="rect">cudaStreamAttachMemAsync</a> to a single stream, the default association, as specifed during <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cudaMallocManaged</a>, is restored when that stream is destroyed. For __managed__ variables, the default association is always <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a>. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen
+                              <p class="p">If the association is later changed via <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" title="Attach memory to a stream asynchronously." shape="rect">cudaStreamAttachMemAsync</a> to a single stream, the default association, as specified during <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cudaMallocManaged</a>, is restored when that stream is destroyed. For __managed__ variables, the default association is always <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a>. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen
                                  until all work in the stream has completed.
                               </p>
                               <p class="p">Memory allocated with <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cudaMallocManaged</a> should be released with <a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094" title="Frees memory on the device." shape="rect">cudaFree</a>.
@@ -1965,7 +1965,7 @@
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">numBlocks</span></tt></dt>
                                  <dd>- Returned occupancy </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">func</span></tt></dt>
-                                 <dd>- Kernel function for which occupancy is calulated </dd>
+                                 <dd>- Kernel function for which occupancy is calculated </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">blockSize</span></tt></dt>
                                  <dd>- Block size the kernel is intended to be launched with </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">dynamicSMemSize</span></tt></dt>
@@ -2003,7 +2003,7 @@
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize</a></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags</a></p>
                               <p class="p"></p>
@@ -2023,7 +2023,7 @@
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">numBlocks</span></tt></dt>
                                  <dd>- Returned occupancy </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">func</span></tt></dt>
-                                 <dd>- Kernel function for which occupancy is calulated </dd>
+                                 <dd>- Kernel function for which occupancy is calculated </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">blockSize</span></tt></dt>
                                  <dd>- Block size the kernel is intended to be launched with </dd>
                                  <dt><tt class="code"><span class="keyword keyword apiItemName">dynamicSMemSize</span></tt></dt>
@@ -2083,7 +2083,7 @@
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize</a></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags</a></p>
                               <p class="p"></p>
@@ -2147,7 +2147,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem</a></p>
@@ -2215,7 +2215,7 @@
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize</a></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -2301,7 +2301,7 @@
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a></p>
                               <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize</a></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -2309,7 +2309,7 @@
                               <!-- --></a><p class="template">template &lt; class T &gt;</p><span><span class="keyword keyword apiItemName">__host__</span>
                               ​<a href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6" title="" shape="rect">cudaError_t</a> cudaOccupancyMaxPotentialBlockSizeWithFlags (  int*<span>&nbsp;</span><span class="keyword keyword apiItemName">minGridSize</span>, int*<span>&nbsp;</span><span class="keyword keyword apiItemName">blockSize</span>, T<span>&nbsp;</span><span class="keyword keyword apiItemName">func</span>, size_t<span>&nbsp;</span><span class="keyword keyword apiItemName">dynamicSMemSize</span> = <span class="ph ph apiData">0</span>, int <span>&nbsp;</span><span class="keyword keyword apiItemName">blockSizeLimit</span> = <span class="ph ph apiData">0</span>, unsigned int <span>&nbsp;</span><span class="keyword keyword apiItemName">flags</span> = <span class="ph ph apiData">0</span> )  [inline] </span></dt>
                         <dd class="description">
-                           <div class="section">Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags. </div>
+                           <div class="section">Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags. </div>
                            <div class="section">
                               <h6 class="parameter_header">
                                  Parameters
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__MEMORY.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__MEMORY.html
@@ -1711,7 +1711,7 @@
                               </p>
                               <p class="p"><tt class="ph tt code">flags</tt> specifies the default stream association for this allocation. <tt class="ph tt code">flags</tt> must be one of <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a> or <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4f9a428d18fdd89a99441d0dd27131c0" shape="rect">cudaMemAttachHost</a>. The default value for <tt class="ph tt code">flags</tt> is <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a>. If <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a> is specified, then this memory is accessible from any stream on any device. If <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4f9a428d18fdd89a99441d0dd27131c0" shape="rect">cudaMemAttachHost</a> is specified, then the allocation should not be accessed from devices that have a zero value for the device attribute <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1gg49e2f8c2c0bd6fe264f2fc970912e5cdc88178f29891f2c18fe67361cc80de09" shape="rect">cudaDevAttrConcurrentManagedAccess</a>; an explicit call to <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" title="Attach memory to a stream asynchronously." shape="rect">cudaStreamAttachMemAsync</a> will be required to enable access on such devices.
                               </p>
-                              <p class="p">If the association is later changed via <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" title="Attach memory to a stream asynchronously." shape="rect">cudaStreamAttachMemAsync</a> to a single stream, the default association, as specifed during <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cudaMallocManaged</a>, is restored when that stream is destroyed. For __managed__ variables, the default association is always <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a>. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen
+                              <p class="p">If the association is later changed via <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g496353d630c29c44a2e33f531a3944d1" title="Attach memory to a stream asynchronously." shape="rect">cudaStreamAttachMemAsync</a> to a single stream, the default association, as specified during <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cudaMallocManaged</a>, is restored when that stream is destroyed. For __managed__ variables, the default association is always <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g4808e47eba73eb94622ec70a9f9b91ff" shape="rect">cudaMemAttachGlobal</a>. Note that destroying a stream is an asynchronous operation, and as a result, the change to default association won't happen
                                  until all work in the stream has completed.
                               </p>
                               <p class="p">Memory allocated with <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e" title="Allocates memory that will be automatically managed by the Unified Memory system." shape="rect">cudaMallocManaged</a> should be released with <a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094" title="Frees memory on the device." shape="rect">cudaFree</a>.
@@ -3077,7 +3077,7 @@
         enum <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g18fa99055ee694244a270e4d5101e95b" shape="rect">cudaMemcpyKind</a>   
                   <a class="xref" href="structcudaMemcpy3DParms.html#structcudaMemcpy3DParms_10caa37ba0134b351170924565d07be20" shape="rect">kind</a>;
       };</pre></p>
-                              <p class="p"><a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gfec7ee5257d48c8528a709ffad48d208" title="Copies data between 3D objects." shape="rect">cudaMemcpy3D()</a> copies data betwen two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA
+                              <p class="p"><a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gfec7ee5257d48c8528a709ffad48d208" title="Copies data between 3D objects." shape="rect">cudaMemcpy3D()</a> copies data between two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA
                                  array. The source, destination, extent, and kind of copy performed is specified by the <a class="xref" href="structcudaMemcpy3DParms.html#structcudaMemcpy3DParms" shape="rect">cudaMemcpy3DParms</a> struct which should be initialized to zero before use: <pre xml:space="preserve">‎<a class="xref" href="structcudaMemcpy3DParms.html#structcudaMemcpy3DParms" shape="rect">cudaMemcpy3DParms</a> myParms = {0};</pre></p>
                               <p class="p">The struct passed to <a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gfec7ee5257d48c8528a709ffad48d208" title="Copies data between 3D objects." shape="rect">cudaMemcpy3D()</a> must specify one of <tt class="ph tt code">srcArray</tt> or <tt class="ph tt code">srcPtr</tt> and one of <tt class="ph tt code">dstArray</tt> or <tt class="ph tt code">dstPtr</tt>. Passing more than one non-zero source or destination will cause <a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gfec7ee5257d48c8528a709ffad48d208" title="Copies data between 3D objects." shape="rect">cudaMemcpy3D()</a> to return an error.
                               </p>
@@ -3188,7 +3188,7 @@
         enum <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1g18fa99055ee694244a270e4d5101e95b" shape="rect">cudaMemcpyKind</a>   
                   <a class="xref" href="structcudaMemcpy3DParms.html#structcudaMemcpy3DParms_10caa37ba0134b351170924565d07be20" shape="rect">kind</a>;
       };</pre></p>
-                              <p class="p"><a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g785bd0963e476a740533382a67674641" title="Copies data between 3D objects." shape="rect">cudaMemcpy3DAsync()</a> copies data betwen two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA
+                              <p class="p"><a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g785bd0963e476a740533382a67674641" title="Copies data between 3D objects." shape="rect">cudaMemcpy3DAsync()</a> copies data between two 3D objects. The source and destination objects may be in either host memory, device memory, or a CUDA
                                  array. The source, destination, extent, and kind of copy performed is specified by the <a class="xref" href="structcudaMemcpy3DParms.html#structcudaMemcpy3DParms" shape="rect">cudaMemcpy3DParms</a> struct which should be initialized to zero before use: <pre xml:space="preserve">‎<a class="xref" href="structcudaMemcpy3DParms.html#structcudaMemcpy3DParms" shape="rect">cudaMemcpy3DParms</a> myParms = {0};</pre></p>
                               <p class="p">The struct passed to <a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g785bd0963e476a740533382a67674641" title="Copies data between 3D objects." shape="rect">cudaMemcpy3DAsync()</a> must specify one of <tt class="ph tt code">srcArray</tt> or <tt class="ph tt code">srcPtr</tt> and one of <tt class="ph tt code">dstArray</tt> or <tt class="ph tt code">dstPtr</tt>. Passing more than one non-zero source or destination will cause <a class="xref" href="group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g785bd0963e476a740533382a67674641" title="Copies data between 3D objects." shape="rect">cudaMemcpy3DAsync()</a> to return an error.
                               </p>
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__OCCUPANCY.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__OCCUPANCY.html
@@ -302,7 +302,7 @@
                      <p>This section describes the occupancy calculation functions of the CUDA runtime application programming interface.</p>
                      <p class="p">Besides the occupancy calculator functions (<a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a> and <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a>), there are also C++ only occupancy-based launch configuration functions documented in <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL" title="C++-style interface built on top of CUDA runtime API." shape="rect">C++ API Routines</a> module.
                      </p>
-                     <p class="p">See <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a></p>
+                     <p class="p">See <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a></p>
                   </div>
                   <h3 class="fake_sectiontitle member_header">Functions</h3>
                   <dl class="members">
@@ -370,7 +370,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags ( C++ API)</a>, <a class="xref" href="../cuda-driver-api/group__CUDA__OCCUPANCY.html#group__CUDA__OCCUPANCY_1gcc6e1094d05cba2cee17fe33ddd04a98" target="_blank" shape="rect">cuOccupancyMaxActiveBlocksPerMultiprocessor</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g603b86b20b37823253ff89fe8688ba83" title="Returns occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags ( C++ API)</a>, <a class="xref" href="../cuda-driver-api/group__CUDA__OCCUPANCY.html#group__CUDA__OCCUPANCY_1gcc6e1094d05cba2cee17fe33ddd04a98" target="_blank" shape="rect">cuOccupancyMaxActiveBlocksPerMultiprocessor</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
@@ -446,7 +446,7 @@
                               </p>
                               <p class="p"></p>
                               <p class="p apiDesc_subtitle"><strong class="ph b">See also:</strong></p>
-                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achived maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags ( C++ API)</a>, <a class="xref" href="../cuda-driver-api/group__CUDA__OCCUPANCY.html#group__CUDA__OCCUPANCY_1g8f1da4d4983e5c3025447665423ae2c2" target="_blank" shape="rect">cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a></p>
+                              <p class="p see_subsection"><a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g5a5d67a3c907371559ba692195e8a38c" title="Returns occupancy for a device function." shape="rect">cudaOccupancyMaxActiveBlocksPerMultiprocessor</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gee5334618ed4bb0871e4559a77643fc1" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSize ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gd0524825c5c01bbc9a5e29e890745800" title="Returns grid and block size that achieved maximum potential occupancy for a device function with the specified flags." shape="rect">cudaOccupancyMaxPotentialBlockSizeWithFlags ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g77b3bfb154b86e215a5bc01509ce8ea6" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMem ( C++ API)</a>, <a class="xref" href="group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g76975517a048cc199bc9e3ea6396ef26" title="Returns grid and block size that achieves maximum potential occupancy for a device function." shape="rect">cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags ( C++ API)</a>, <a class="xref" href="../cuda-driver-api/group__CUDA__OCCUPANCY.html#group__CUDA__OCCUPANCY_1g8f1da4d4983e5c3025447665423ae2c2" target="_blank" shape="rect">cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags</a></p>
                               <p class="p"></p>
                            </div>
                         </dd>
--- a/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__TYPES.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/group__CUDART__TYPES.html
@@ -2383,7 +2383,7 @@
                               </h6>
                               <dl class="enumerator">
                                  <dt><span class="enum-member-name-def">cudaMemRangeAttributeReadMostly = <span class="ph ph apiData">1</span></span></dt>
-                                 <dd>Whether the range will mostly be read and only occassionally be written to </dd>
+                                 <dd>Whether the range will mostly be read and only occasionally be written to </dd>
                                  <dt><span class="enum-member-name-def">cudaMemRangeAttributePreferredLocation = <span class="ph ph apiData">2</span></span></dt>
                                  <dd>The preferred location of the range </dd>
                                  <dt><span class="enum-member-name-def">cudaMemRangeAttributeAccessedBy = <span class="ph ph apiData">3</span></span></dt>
@@ -2432,7 +2432,7 @@
                               </h6>
                               <dl class="enumerator">
                                  <dt><span class="enum-member-name-def">cudaMemAdviseSetReadMostly = <span class="ph ph apiData">1</span></span></dt>
-                                 <dd>Data will mostly be read and only occassionally be written to </dd>
+                                 <dd>Data will mostly be read and only occasionally be written to </dd>
                                  <dt><span class="enum-member-name-def">cudaMemAdviseUnsetReadMostly = <span class="ph ph apiData">2</span></span></dt>
                                  <dd>Undo the effect of <a class="xref" href="group__CUDART__TYPES.html#group__CUDART__TYPES_1ggc314a8b14091f7e02a7ad15dcb36c857441d911811beda174627f403142d5ff0" shape="rect">cudaMemAdviseSetReadMostly</a></dd>
                                  <dt><span class="enum-member-name-def">cudaMemAdviseSetPreferredLocation = <span class="ph ph apiData">3</span></span></dt>
--- a/nvidia-cuda/doc/html/cuda-runtime-api/structcudaLaunchParams.html
+++ b/nvidia-cuda/doc/html/cuda-runtime-api/structcudaLaunchParams.html
@@ -341,7 +341,7 @@
                               <!-- --></a><span>dim3  <a href="structcudaLaunchParams.html#structcudaLaunchParams" title="" shape="rect">cudaLaunchParams</a>::<a href="structcudaLaunchParams.html#structcudaLaunchParams_1dd319c6a16b02bdcce0c08fb3e03b87f" shape="rect">blockDim</a> [inherited] </span></dt>
                         <dd class="description">
                            <div class="section">
-                              <p> Block dimentions </p>
+                              <p> Block dimensions </p>
                            </div>
                         </dd>
                         <dt class="description"><a name="structcudaLaunchParams_1b83c5ba1757e15e87386300690c9b081" id="structcudaLaunchParams_1b83c5ba1757e15e87386300690c9b081" shape="rect">
@@ -358,7 +358,7 @@
                               <!-- --></a><span>dim3  <a href="structcudaLaunchParams.html#structcudaLaunchParams" title="" shape="rect">cudaLaunchParams</a>::<a href="structcudaLaunchParams.html#structcudaLaunchParams_1ad6b5e617a8c137476babb2ca1b94570" shape="rect">gridDim</a> [inherited] </span></dt>
                         <dd class="description">
                            <div class="section">
-                              <p> Grid dimentions </p>
+                              <p> Grid dimensions </p>
                            </div>
                         </dd>
                         <dt class="description"><a name="structcudaLaunchParams_186f6dff594ff6d496509a9f3c6607f17" id="structcudaLaunchParams_186f6dff594ff6d496509a9f3c6607f17" shape="rect">
--- a/nvidia-cuda/doc/html/profiler-users-guide/index.html
+++ b/nvidia-cuda/doc/html/profiler-users-guide/index.html
@@ -4215,7 +4215,7 @@ Critical path(%)  Critical path  Waiting
                         <!-- --></a><h3 class="title topictitle2"><a href="#openacc" name="openacc" shape="rect">3.6.&nbsp;OpenACC</a></h3>
                      <div class="body conbody">
                         <p class="p">On 64bit Linux platforms, <samp class="ph codeph"><span class="keyword">nvprof</span></samp> supports recording OpenACC activities
-                           using the CUPTI Activity API.  This allows to investigate the
+                           using the CUPTI Activity API.  This allows one to investigate the
                            performance on the level of OpenACC constructs in addition to the
                            underlying, compiler-generated CUDA API calls.  
                         </p>
@@ -5307,7 +5307,7 @@ $ mpirun -np 2 -host c0-0,c0-1 <span cla
                   <div class="body conbody">
                      <p class="p">The dependency analysis feature enables optimization of the program
                         runtime and concurrency of applications utilizing multiple CPU threads and
-                        CUDA streams. It allows to compute the critical path of a specific
+                        CUDA streams. It allows one to compute the critical path of a specific
                         execution, detect waiting time and inspect dependencies between functions
                         executing in different threads or streams.
                      </p>
@@ -10221,7 +10221,7 @@ $ mpirun -np 2 -host c0-0,c0-1 <span cla
                         <div class="p">List of changes done as part of the CUDA Toolkit 9.2 release.
                            
                            <ul class="ul">
-                              <li class="li">The <span class="keyword">Visual Profiler</span> allows to switch multiple segments to non-segment mode
+                              <li class="li">The <span class="keyword">Visual Profiler</span> allows one to switch multiple segments to non-segment mode
                                  for Unified Memory profiling on the timeline. Earlier it was restircted to single segment only.
                               </li>
                               <li class="li">The <span class="keyword">Visual Profiler</span> shows a summary view of the memory hierarchy
@@ -10255,7 +10255,7 @@ $ mpirun -np 2 -host c0-0,c0-1 <span cla
                                     </li>
                                  </ul>
                               </li>
-                              <li class="li"><span class="keyword">nvprof</span> allows to collect tracing infromation along with the profiling information
+                              <li class="li"><span class="keyword">nvprof</span> allows one to collect tracing infromation along with the profiling information
                                  in the same pass. Use new option <samp class="ph codeph">--trace &lt;api|gpu&gt;</samp> to enable trace along with
                                  collection of events/metrics.
                               </li>
--- a/nvidia-cuda/doc/man/man3/CUDART_D3D9.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_D3D9.3
@@ -151,7 +151,7 @@ If \fCdevice\fP has already been initial
 .PP
 Successfully initializing CUDA interoperability with \fCpD3D9Device\fP will increase the internal reference count on \fCpD3D9Device\fP. This reference count will be decremented when \fCdevice\fP is reset using \fBcudaDeviceReset()\fP.
 .PP
-Note that this function is never required for correct functionality. Use of this function will result in accelerated interoperability only when the operating system is Windows Vista or Windows 7, and the device \fCpD3DDdevice\fP is not an IDirect3DDevice9Ex. In all other cirumstances, this function is not necessary.
+Note that this function is never required for correct functionality. Use of this function will result in accelerated interoperability only when the operating system is Windows Vista or Windows 7, and the device \fCpD3DDdevice\fP is not an IDirect3DDevice9Ex. In all other circumstances, this function is not necessary.
 .PP
 \fBParameters:\fP
 .RS 4
--- a/nvidia-cuda/doc/man/man3/CUDART_EGL.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_EGL.3
@@ -142,7 +142,7 @@ Disconnect CUDA as a consumer to EGLStre
 .PP
 \fBParameters:\fP
 .RS 4
-\fIconn\fP - Conection to disconnect.
+\fIconn\fP - Connection to disconnect.
 .RE
 .PP
 \fBReturns:\fP
@@ -214,7 +214,7 @@ Disconnect CUDA as a producer to EGLStre
 .PP
 \fBParameters:\fP
 .RS 4
-\fIconn\fP - Conection to disconnect.
+\fIconn\fP - Connection to disconnect.
 .RE
 .PP
 \fBReturns:\fP
@@ -330,7 +330,7 @@ The EGLSyncKHR is an opaque handle to an
 .PP
 Registers the EGLImageKHR specified by \fCimage\fP for access by CUDA. A handle to the registered object is returned as \fCpCudaResource\fP. Additional Mapping/Unmapping is not required for the registered resource and \fBcudaGraphicsResourceGetMappedEglFrame\fP can be directly called on the \fCpCudaResource\fP.
 .PP
-The application will be responsible for synchronizing access to shared objects. The application must ensure that any pending operation which access the objects have completed before passing control to CUDA. This may be accomplished by issuing and waiting for glFinish command on all GLcontexts (for OpenGL and likewise for other APIs). The application will be also responsible for ensuring that any pending operation on the registered CUDA resource has completed prior to executing subsequent commands in other APIs accesing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize (preferably).
+The application will be responsible for synchronizing access to shared objects. The application must ensure that any pending operation which access the objects have completed before passing control to CUDA. This may be accomplished by issuing and waiting for glFinish command on all GLcontexts (for OpenGL and likewise for other APIs). The application will be also responsible for ensuring that any pending operation on the registered CUDA resource has completed prior to executing subsequent commands in other APIs accessing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize (preferably).
 .PP
 The surface's intended usage is specified using \fCflags\fP, as follows:
 .PP
--- a/nvidia-cuda/doc/man/man3/CUDART_TYPES.3
+++ b/nvidia-cuda/doc/man/man3/CUDART_TYPES.3
@@ -1951,7 +1951,7 @@ CUDA Memory Advise values
 .in +1c
 .TP
 \fB\fIcudaMemAdviseSetReadMostly \fP\fP
-Data will mostly be read and only occassionally be written to 
+Data will mostly be read and only occasionally be written to 
 .TP
 \fB\fIcudaMemAdviseUnsetReadMostly \fP\fP
 Undo the effect of \fBcudaMemAdviseSetReadMostly\fP 
@@ -1993,7 +1993,7 @@ CUDA range attributes
 .in +1c
 .TP
 \fB\fIcudaMemRangeAttributeReadMostly \fP\fP
-Whether the range will mostly be read and only occassionally be written to 
+Whether the range will mostly be read and only occasionally be written to 
 .TP
 \fB\fIcudaMemRangeAttributePreferredLocation \fP\fP
 The preferred location of the range 
--- a/nvidia-cuda/doc/man/man3/CUDA_DEVICE_DEPRECATED.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_DEVICE_DEPRECATED.3
@@ -28,7 +28,7 @@ This section describes the device manage
 .RS 4
 .RE
 .PP
-This function was deprecated as of CUDA 5.0 and its functionality superceded by \fBcuDeviceGetAttribute()\fP.
+This function was deprecated as of CUDA 5.0 and its functionality superseded by \fBcuDeviceGetAttribute()\fP.
 .PP
 Returns in \fC*major\fP and \fC*minor\fP the major and minor revision numbers that define the compute capability of the device \fCdev\fP.
 .PP
--- a/nvidia-cuda/doc/man/man3/CUDA_EGL.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_EGL.3
@@ -146,7 +146,7 @@ The EGLStreamKHR is an EGL object that t
 .PP
 \fBParameters:\fP
 .RS 4
-\fIconn\fP - Conection to disconnect.
+\fIconn\fP - Connection to disconnect.
 .RE
 .PP
 \fBReturns:\fP
@@ -222,7 +222,7 @@ The EGLStreamKHR is an EGL object that t
 .PP
 \fBParameters:\fP
 .RS 4
-\fIconn\fP - Conection to disconnect.
+\fIconn\fP - Connection to disconnect.
 .RE
 .PP
 \fBReturns:\fP
@@ -349,7 +349,7 @@ The EGLSyncKHR is an opaque handle to an
 .PP
 Registers the EGLImageKHR specified by \fCimage\fP for access by CUDA. A handle to the registered object is returned as \fCpCudaResource\fP. Additional Mapping/Unmapping is not required for the registered resource and \fBcuGraphicsResourceGetMappedEglFrame\fP can be directly called on the \fCpCudaResource\fP.
 .PP
-The application will be responsible for synchronizing access to shared objects. The application must ensure that any pending operation which access the objects have completed before passing control to CUDA. This may be accomplished by issuing and waiting for glFinish command on all GLcontexts (for OpenGL and likewise for other APIs). The application will be also responsible for ensuring that any pending operation on the registered CUDA resource has completed prior to executing subsequent commands in other APIs accesing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize (preferably).
+The application will be responsible for synchronizing access to shared objects. The application must ensure that any pending operation which access the objects have completed before passing control to CUDA. This may be accomplished by issuing and waiting for glFinish command on all GLcontexts (for OpenGL and likewise for other APIs). The application will be also responsible for ensuring that any pending operation on the registered CUDA resource has completed prior to executing subsequent commands in other APIs accessing the same memory objects. This can be accomplished by calling cuCtxSynchronize or cuEventSynchronize (preferably).
 .PP
 The surface's intended usage is specified using \fCflags\fP, as follows:
 .PP
--- a/nvidia-cuda/doc/man/man3/cudaLaunchParams.3
+++ b/nvidia-cuda/doc/man/man3/cudaLaunchParams.3
@@ -38,13 +38,13 @@ CUDA launch parameters
 Arguments 
 .SS "dim3 \fBcudaLaunchParams::blockDim\fP"
 .PP
-Block dimentions 
+Block dimensions 
 .SS "void* \fBcudaLaunchParams::func\fP"
 .PP
 Device function symbol 
 .SS "dim3 \fBcudaLaunchParams::gridDim\fP"
 .PP
-Grid dimentions 
+Grid dimensions 
 .SS "size_t \fBcudaLaunchParams::sharedMem\fP"
 .PP
 Shared memory 
--- a/nvidia-cuda/doc/man/man3/nvmlDeviceEnumvs.3
+++ b/nvidia-cuda/doc/man/man3/nvmlDeviceEnumvs.3
@@ -227,7 +227,7 @@ Feature enabled.
 .PP
 GPU Operation Mode
 .PP
-GOM allows to reduce power usage and optimize GPU throughput by disabling GPU features.
+GOM allows one to reduce power usage and optimize GPU throughput by disabling GPU features.
 .PP
 Each GOM is designed to meet specific user needs. 
 .PP
@@ -470,7 +470,7 @@ Insufficient memory.
 No data. 
 .TP
 \fB\fINVML_ERROR_VGPU_ECC_NOT_SUPPORTED \fP\fP
-The requested vgpu operation is not available on target device, becasue ECC is enabled. 
+The requested vgpu operation is not available on target device, because ECC is enabled. 
 .TP
 \fB\fINVML_ERROR_UNKNOWN \fP\fP
 An internal driver error occurred. 
--- a/nvidia-cuda/doc/man/man3/nvmlDeviceQueries.3
+++ b/nvidia-cuda/doc/man/man3/nvmlDeviceQueries.3
@@ -1250,7 +1250,7 @@ Retrieves information about active encod
 .PP
 An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \fIsessionInfos\fP. The array elememt count is passed in \fIsessionCount\fP, and \fIsessionCount\fP is used to return the number of sessions written to the buffer.
 .PP
-If the supplied buffer is not large enough to accomodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlEncoderSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
+If the supplied buffer is not large enough to accommodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlEncoderSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
 .PP
 For Maxwell (TM) or newer fully supported devices.
 .PP
@@ -1458,7 +1458,7 @@ Retrieves information about active frame
 .PP
 An array of active FBC sessions is returned in the caller-supplied buffer pointed at by \fIsessionInfo\fP. The array element count is passed in \fIsessionCount\fP, and \fIsessionCount\fP is used to return the number of sessions written to the buffer.
 .PP
-If the supplied buffer is not large enough to accomodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlFBCSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active FBC sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.
+If the supplied buffer is not large enough to accommodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlFBCSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active FBC sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.
 .PP
 For Maxwell (TM) or newer fully supported devices.
 .PP
@@ -1504,7 +1504,7 @@ For Maxwell (TM) or newer fully supporte
 .RS 4
 \fIdevice\fP The identifier of the target device 
 .br
-\fIfbcStats\fP Reference to \fBnvmlFBCStats_t\fP structure contianing NvFBC stats
+\fIfbcStats\fP Reference to \fBnvmlFBCStats_t\fP structure containing NvFBC stats
 .RE
 .PP
 \fBReturns:\fP
@@ -2751,7 +2751,7 @@ Returns the list of retired pages by sou
 .PP
 \fBNote:\fP
 .RS 4
-nvmlDeviceGetRetiredPages_v2 adds an additional timestamps paramter to return the time of each page's retirement.
+nvmlDeviceGetRetiredPages_v2 adds an additional timestamps parameter to return the time of each page's retirement.
 .RE
 .PP
 For Kepler (TM) or newer fully supported devices.
--- a/nvidia-cuda/doc/man/man3/nvmlVgpu.3
+++ b/nvidia-cuda/doc/man/man3/nvmlVgpu.3
@@ -99,7 +99,7 @@ Retrieve the active vGPU instances on a
 .PP
 An array of active vGPU instances is returned in the caller-supplied buffer pointed at by \fIvgpuInstances\fP. The array elememt count is passed in \fIvgpuCount\fP, and \fIvgpuCount\fP is used to return the number of vGPU instances written to the buffer.
 .PP
-If the supplied buffer is not large enough to accomodate the vGPU instance array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuInstance_t array required in \fIvgpuCount\fP. To query the number of active vGPU instances, call this function with *vgpuCount = 0. The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU Types are supported.
+If the supplied buffer is not large enough to accommodate the vGPU instance array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuInstance_t array required in \fIvgpuCount\fP. To query the number of active vGPU instances, call this function with *vgpuCount = 0. The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU Types are supported.
 .PP
 For Kepler (TM) or newer fully supported devices.
 .PP
@@ -138,7 +138,7 @@ An array of creatable vGPU types for the
 .PP
 The creatable vGPU types for a device may differ over time, as there may be restrictions on what type of vGPU types can concurrently run on a device. For example, if only one vGPU type is allowed at a time on a device, then the creatable list will be restricted to whatever vGPU type is already running on the device.
 .PP
-If the supplied buffer is not large enough to accomodate the vGPU type array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \fIvgpuCount\fP. To query the number of vGPU types createable for the GPU, call this function with *vgpuCount = 0. The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are creatable.
+If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \fIvgpuCount\fP. To query the number of vGPU types createable for the GPU, call this function with *vgpuCount = 0. The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are creatable.
 .PP
 \fBParameters:\fP
 .RS 4
@@ -171,7 +171,7 @@ Retrieve the supported vGPU types on a p
 .PP
 An array of supported vGPU types for the physical GPU indicated by \fIdevice\fP is returned in the caller-supplied buffer pointed at by \fIvgpuTypeIds\fP. The element count of nvmlVgpuTypeId_t array is passed in \fIvgpuCount\fP, and \fIvgpuCount\fP is used to return the number of vGPU types written to the buffer.
 .PP
-If the supplied buffer is not large enough to accomodate the vGPU type array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \fIvgpuCount\fP. To query the number of vGPU types supported for the GPU, call this function with *vgpuCount = 0. The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are supported.
+If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \fIvgpuCount\fP. To query the number of vGPU types supported for the GPU, call this function with *vgpuCount = 0. The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are supported.
 .PP
 \fBParameters:\fP
 .RS 4
@@ -243,7 +243,7 @@ For Maxwell (TM) or newer fully supporte
 \fBReturns:\fP
 .RS 4
 .IP "\(bu" 2
-\fBNVML_SUCCESS\fP if \fIencoderCapacity\fP has been retrived
+\fBNVML_SUCCESS\fP if \fIencoderCapacity\fP has been retrieved
 .IP "\(bu" 2
 \fBNVML_ERROR_UNINITIALIZED\fP if the library has not been successfully initialized
 .IP "\(bu" 2
@@ -262,7 +262,7 @@ Retrieves information about all active e
 .PP
 An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \fIsessionInfo\fP. The array element count is passed in \fIsessionCount\fP, and \fIsessionCount\fP is used to return the number of sessions written to the buffer.
 .PP
-If the supplied buffer is not large enough to accomodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlEncoderSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
+If the supplied buffer is not large enough to accommodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlEncoderSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
 .PP
 For Maxwell (TM) or newer fully supported devices.
 .PP
@@ -332,7 +332,7 @@ Retrieves information about active frame
 .PP
 An array of active FBC sessions is returned in the caller-supplied buffer pointed at by \fIsessionInfo\fP. The array element count is passed in \fIsessionCount\fP, and \fIsessionCount\fP is used to return the number of sessions written to the buffer.
 .PP
-If the supplied buffer is not large enough to accomodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlFBCSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active FBC sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.
+If the supplied buffer is not large enough to accommodate the active session array, the function returns NVML_ERROR_INSUFFICIENT_SIZE, with the element count of \fBnvmlFBCSessionInfo_t\fP array required in \fIsessionCount\fP. To query the number of active FBC sessions, call this function with *sessionCount = 0. The code will return NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.
 .PP
 For Maxwell (TM) or newer fully supported devices.
 .PP
@@ -378,7 +378,7 @@ For Maxwell (TM) or newer fully supporte
 .RS 4
 \fIvgpuInstance\fP Identifier of the target vGPU instance 
 .br
-\fIfbcStats\fP Reference to \fBnvmlFBCStats_t\fP structure contianing NvFBC stats
+\fIfbcStats\fP Reference to \fBnvmlFBCStats_t\fP structure containing NvFBC stats
 .RE
 .PP
 \fBReturns:\fP
@@ -704,7 +704,7 @@ For Kepler (TM) or newer fully supported
 .br
 \fIdeviceID\fP Device ID and vendor ID of the device contained in single 32 bit value 
 .br
-\fIsubsystemID\fP Subsytem ID and subsytem vendor ID of the device contained in single 32 bit value
+\fIsubsystemID\fP Subsystem ID and subsystem vendor ID of the device contained in single 32 bit value
 .RE
 .PP
 \fBReturns:\fP
--- a/nvidia-cuda/extras/CUPTI/include/cupti_callbacks.h
+++ b/nvidia-cuda/extras/CUPTI/include/cupti_callbacks.h
@@ -574,7 +574,7 @@ CUptiResult CUPTIAPI cuptiSupportedDomai
  * \param subscriber Returns handle to initialize subscriber
  * \param callback The callback function
  * \param userdata A pointer to user data. This data will be passed to
- * the callback function via the \p userdata paramater.
+ * the callback function via the \p userdata parameter.
  *
  * \retval CUPTI_SUCCESS on success
  * \retval CUPTI_ERROR_NOT_INITIALIZED if unable to initialize CUPTI
--- a/nvidia-cuda/include/cublasXt.h
+++ b/nvidia-cuda/include/cublasXt.h
@@ -73,7 +73,7 @@ cublasStatus_t CUBLASWINAPI cublasXtMaxB
 /* This routine selects the Gpus that the user want to use for CUBLAS-XT */
 cublasStatus_t CUBLASWINAPI cublasXtDeviceSelect(cublasXtHandle_t handle, int nbDevices, int deviceId[]);
 
-/* This routine allows to change the dimension of the tiles ( blockDim x blockDim ) */
+/* This routine allows one to change the dimension of the tiles ( blockDim x blockDim ) */
 cublasStatus_t CUBLASWINAPI cublasXtSetBlockDim(cublasXtHandle_t handle, int blockDim);
 cublasStatus_t CUBLASWINAPI cublasXtGetBlockDim(cublasXtHandle_t handle, int *blockDim);
 
@@ -81,7 +81,7 @@ typedef enum {
     CUBLASXT_PINNING_DISABLED   = 0,  
     CUBLASXT_PINNING_ENABLED    = 1        
 } cublasXtPinnedMemMode_t;
-/* This routine allows to CUBLAS-XT to pin the Host memory if it find out that some of the matrix passed
+/* This routine allows one to CUBLAS-XT to pin the Host memory if it find out that some of the matrix passed
    are not pinned : Pinning/Unpinning the Host memory is still a costly operation
    It is better if the user controls the memory on its own (by pinning/unpinning oly when necessary)
 */
--- a/nvidia-cuda/include/driver_types.h
+++ b/nvidia-cuda/include/driver_types.h
@@ -1494,7 +1494,7 @@ enum __device_builtin__ cudaLimit
  */
 enum __device_builtin__ cudaMemoryAdvise
 {
-    cudaMemAdviseSetReadMostly          = 1, /**< Data will mostly be read and only occassionally be written to */
+    cudaMemAdviseSetReadMostly          = 1, /**< Data will mostly be read and only occasionally be written to */
     cudaMemAdviseUnsetReadMostly        = 2, /**< Undo the effect of ::cudaMemAdviseSetReadMostly */
     cudaMemAdviseSetPreferredLocation   = 3, /**< Set the preferred location for the data as the specified device */
     cudaMemAdviseUnsetPreferredLocation = 4, /**< Clear the preferred location for the data */
@@ -1507,7 +1507,7 @@ enum __device_builtin__ cudaMemoryAdvise
  */
 enum __device_builtin__ cudaMemRangeAttribute
 {
-    cudaMemRangeAttributeReadMostly           = 1, /**< Whether the range will mostly be read and only occassionally be written to */
+    cudaMemRangeAttributeReadMostly           = 1, /**< Whether the range will mostly be read and only occasionally be written to */
     cudaMemRangeAttributePreferredLocation    = 2, /**< The preferred location of the range */
     cudaMemRangeAttributeAccessedBy           = 3, /**< Memory range has ::cudaMemAdviseSetAccessedBy set for specified device */
     cudaMemRangeAttributeLastPrefetchLocation = 4  /**< The last location to which the range was prefetched */
@@ -2263,8 +2263,8 @@ enum __device_builtin__ cudaCGScope {
 struct __device_builtin__ cudaLaunchParams
 {
     void *func;          /**< Device function symbol */
-    dim3 gridDim;        /**< Grid dimentions */
-    dim3 blockDim;       /**< Block dimentions */
+    dim3 gridDim;        /**< Grid dimensions */
+    dim3 blockDim;       /**< Block dimensions */
     void **args;         /**< Arguments */
     size_t sharedMem;    /**< Shared memory */
     cudaStream_t stream; /**< Stream identifier */
--- a/nvidia-cuda/include/nvml.h
+++ b/nvidia-cuda/include/nvml.h
@@ -682,7 +682,7 @@ typedef enum nvmlPStates_enum
 /**
  * GPU Operation Mode
  *
- * GOM allows to reduce power usage and optimize GPU throughput by disabling GPU features.
+ * GOM allows one to reduce power usage and optimize GPU throughput by disabling GPU features.
  *
  * Each GOM is designed to meet specific user needs.
  */
@@ -738,7 +738,7 @@ typedef enum nvmlReturn_enum
     NVML_ERROR_IN_USE = 19,             //!< An operation cannot be performed because the GPU is currently in use
     NVML_ERROR_MEMORY = 20,             //!< Insufficient memory
     NVML_ERROR_NO_DATA = 21,            //!<No data
-    NVML_ERROR_VGPU_ECC_NOT_SUPPORTED = 22,    //!< The requested vgpu operation is not available on target device, becasue ECC is enabled
+    NVML_ERROR_VGPU_ECC_NOT_SUPPORTED = 22,    //!< The requested vgpu operation is not available on target device, because ECC is enabled
     NVML_ERROR_UNKNOWN = 999            //!< An internal driver error occurred
 } nvmlReturn_t;
 
@@ -1748,7 +1748,7 @@ nvmlReturn_t DECLDIR nvmlSystemGetNVMLVe
  *
  * For all products.
  *
- * The CUDA driver version returned will be retreived from the currently installed version of CUDA.
+ * The CUDA driver version returned will be retrieved from the currently installed version of CUDA.
  * If the cuda library is not found, this function will return a known supported version number.
  *
  * @param cudaDriverVersion                    Reference in which to return the version identifier
@@ -3749,7 +3749,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetEncode
  * array elememt count is passed in \a sessionCount, and \a sessionCount is used to return the number of sessions
  * written to the buffer.
  *
- * If the supplied buffer is not large enough to accomodate the active session array, the function returns
+ * If the supplied buffer is not large enough to accommodate the active session array, the function returns
  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlEncoderSessionInfo_t array required in \a sessionCount.
  * To query the number of active encoder sessions, call this function with *sessionCount = 0.  The code will return
  * NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
@@ -3795,7 +3795,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetDecode
 * For Maxwell &tm; or newer fully supported devices.
 *
 * @param device                            The identifier of the target device
-* @param fbcStats                          Reference to nvmlFBCStats_t structure contianing NvFBC stats
+* @param fbcStats                          Reference to nvmlFBCStats_t structure containing NvFBC stats
 *
 * @return
 *         - \ref NVML_SUCCESS                  if \a fbcStats is fetched
@@ -5443,7 +5443,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetProces
  * pointed at by \a vgpuTypeIds. The element count of nvmlVgpuTypeId_t array is passed in \a vgpuCount, and \a vgpuCount
  * is used to return the number of vGPU types written to the buffer.
  *
- * If the supplied buffer is not large enough to accomodate the vGPU type array, the function returns
+ * If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns
  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \a vgpuCount.
  * To query the number of vGPU types supported for the GPU, call this function with *vgpuCount = 0.
  * The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are supported.
@@ -5472,7 +5472,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetSuppor
  * can concurrently run on a device.  For example, if only one vGPU type is allowed at a time on a device, then the creatable
  * list will be restricted to whatever vGPU type is already running on the device.
  *
- * If the supplied buffer is not large enough to accomodate the vGPU type array, the function returns
+ * If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns
  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \a vgpuCount.
  * To query the number of vGPU types createable for the GPU, call this function with *vgpuCount = 0.
  * The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are creatable.
@@ -5536,7 +5536,7 @@ nvmlReturn_t DECLDIR nvmlVgpuTypeGetName
  *
  * @param vgpuTypeId               Handle to vGPU type
  * @param deviceID                 Device ID and vendor ID of the device contained in single 32 bit value
- * @param subsystemID              Subsytem ID and subsytem vendor ID of the device contained in single 32 bit value
+ * @param subsystemID              Subsystem ID and subsystem vendor ID of the device contained in single 32 bit value
  *
  * @return
  *         - \ref NVML_SUCCESS                 successful completion
@@ -5678,7 +5678,7 @@ nvmlReturn_t DECLDIR nvmlVgpuTypeGetMaxI
  * array elememt count is passed in \a vgpuCount, and \a vgpuCount is used to return the number of vGPU instances
  * written to the buffer.
  *
- * If the supplied buffer is not large enough to accomodate the vGPU instance array, the function returns
+ * If the supplied buffer is not large enough to accommodate the vGPU instance array, the function returns
  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuInstance_t array required in \a vgpuCount.
  * To query the number of active vGPU instances, call this function with *vgpuCount = 0.  The code will return
  * NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU Types are supported.
@@ -5877,7 +5877,7 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGet
  * @param encoderCapacity          Reference to an unsigned int for the encoder capacity
  *
  * @return
- *         - \ref NVML_SUCCESS                 if \a encoderCapacity has been retrived
+ *         - \ref NVML_SUCCESS                 if \a encoderCapacity has been retrieved
  *         - \ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized
  *         - \ref NVML_ERROR_INVALID_ARGUMENT  if \a vgpuInstance is 0, or \a encoderQueryType is invalid
  *         - \ref NVML_ERROR_NOT_FOUND         if \a vgpuInstance does not match a valid active vGPU instance on the system
@@ -5930,7 +5930,7 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGet
  * array element count is passed in \a sessionCount, and \a sessionCount is used to return the number of sessions
  * written to the buffer.
  *
- * If the supplied buffer is not large enough to accomodate the active session array, the function returns
+ * If the supplied buffer is not large enough to accommodate the active session array, the function returns
  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlEncoderSessionInfo_t array required in \a sessionCount.
  * To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return
  * NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.
@@ -5960,7 +5960,7 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGet
 * For Maxwell &tm; or newer fully supported devices.
 *
 * @param vgpuInstance                      Identifier of the target vGPU instance
-* @param fbcStats                          Reference to nvmlFBCStats_t structure contianing NvFBC stats
+* @param fbcStats                          Reference to nvmlFBCStats_t structure containing NvFBC stats
 *
 * @return
 *         - \ref NVML_SUCCESS                  if \a fbcStats is fetched
@@ -6031,7 +6031,7 @@ typedef struct nvmlVgpuMetadata_st
     char                     guestDriverVersion[NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE]; //!< Version of driver installed in guest
     char                     hostDriverVersion[NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE];  //!< Version of driver installed in host
     unsigned int             reserved[6];                                                //!< Reserved for internal use
-    unsigned int             vgpuVirtualizationCaps;                                     //!< vGPU virtualizaion capabilities bitfileld
+    unsigned int             vgpuVirtualizationCaps;                                     //!< vGPU virtualization capabilities bitfield
     unsigned int             guestVgpuVersion;                                           //!< vGPU version of guest driver
     unsigned int             opaqueDataSize;                                             //!< Size of opaque data field in bytes
     char                     opaqueData[4];                                              //!< Opaque data
@@ -6045,7 +6045,7 @@ typedef struct nvmlVgpuPgpuMetadata_st
     unsigned int            version;                                                    //!< Current version of the structure
     unsigned int            revision;                                                   //!< Current revision of the structure
     char                    hostDriverVersion[NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE];  //!< Host driver version
-    unsigned int            pgpuVirtualizationCaps;                                     //!< Pgpu virtualizaion capabilities bitfileld
+    unsigned int            pgpuVirtualizationCaps;                                     //!< Pgpu virtualization capabilities bitfield
     unsigned int            reserved[5];                                                //!< Reserved for internal use
     nvmlVgpuVersion_t       hostSupportedVgpuRange;                                     //!< vGPU version range supported by host driver
     unsigned int            opaqueDataSize;                                             //!< Size of opaque data field in bytes
@@ -6143,7 +6143,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetVgpuMe
  *
  * The caller passes in a buffer via \a compatibilityInfo, into which a compatibility information structure is written. The
  * structure defines the states in which the vGPU / VM may be booted on the physical GPU. If the vGPU / VM compatibility
- * with the physical GPU is limited, a limit code indicates the factor limiting compability.
+ * with the physical GPU is limited, a limit code indicates the factor limiting compatibility.
  * (see \ref nvmlVgpuPgpuCompatibilityLimitCode_t for details).
  *
  * Note: vGPU compatibility does not take into account dynamic capacity conditions that may limit a system's ability to
--- a/nvidia-cuda/libnsight/plugins/org.eclipse.ui.intro.universal_3.2.700.v20130904-1701/themes/purpleMesh/html/root.css
+++ b/nvidia-cuda/libnsight/plugins/org.eclipse.ui.intro.universal_3.2.700.v20130904-1701/themes/purpleMesh/html/root.css
@@ -60,7 +60,7 @@ body {
 }
 /* specify a width for Moz so we can center.  
  * **Important** If additional links are added, we will have to increase this width 
- * to accomodate them, otherwise they will wrap to a new line 
+ * to accommodate them, otherwise they will wrap to a new line 
  */
 
 #links-background > #page-links {
--- a/nvidia-cuda/doc/man/man3/nvml.3
+++ b/nvidia-cuda/doc/man/man3/nvml.3
@@ -166,7 +166,7 @@ The caller passes in a buffer via \fIpgp
 .PP
 Takes a vGPU instance metadata structure read from \fBnvmlVgpuInstanceGetMetadata()\fP, and a vGPU metadata structure for a physical GPU read from \fBnvmlDeviceGetVgpuMetadata()\fP, and returns compatibility information of the vGPU instance and the physical GPU.
 .PP
-The caller passes in a buffer via \fIcompatibilityInfo\fP, into which a compatibility information structure is written. The structure defines the states in which the vGPU / VM may be booted on the physical GPU. If the vGPU / VM compatibility with the physical GPU is limited, a limit code indicates the factor limiting compability. (see \fBnvmlVgpuPgpuCompatibilityLimitCode_t\fP for details).
+The caller passes in a buffer via \fIcompatibilityInfo\fP, into which a compatibility information structure is written. The structure defines the states in which the vGPU / VM may be booted on the physical GPU. If the vGPU / VM compatibility with the physical GPU is limited, a limit code indicates the factor limiting compatibility. (see \fBnvmlVgpuPgpuCompatibilityLimitCode_t\fP for details).
 .PP
 Note: vGPU compatibility does not take into account dynamic capacity conditions that may limit a system's ability to boot a given vGPU or associated VM.
 .PP
--- a/nvidia-cuda/doc/html/cusolver/index.html
+++ b/nvidia-cuda/doc/html/cusolver/index.html
@@ -12958,7 +12958,7 @@ cusolverDnZgesvdaStridedBatched(
                                  </table>
                               </div>
                               <p class="p">
-                                 if the paramter <samp class="ph codeph">rank</samp> is equal <samp class="ph codeph">n</samp>.
+                                 if the parameter <samp class="ph codeph">rank</samp> is equal <samp class="ph codeph">n</samp>.
                                  Otherwise, <samp class="ph codeph">h_RnrmF</samp> reports  
                                  
                               </p>
@@ -19158,7 +19158,7 @@ cusolverSpZcsreigvsi(cusolverSpHandle_t
                               <p class="p"><samp class="ph codeph">A</samp> is an <samp class="ph codeph">m×m</samp> sparse matrix that is defined in CSR storage
                                  format by the three arrays <samp class="ph codeph">csrValA</samp>, <samp class="ph codeph">csrRowPtrA</samp>, and
                                  <samp class="ph codeph">csrColIndA</samp>.
-                                 The output paramter <samp class="ph codeph">x</samp> is the approximated eigenvector of size <samp class="ph codeph">m</samp>,
+                                 The output parameter <samp class="ph codeph">x</samp> is the approximated eigenvector of size <samp class="ph codeph">m</samp>,
                                  
                               </p>
                               <p class="p">
--- a/nvidia-cuda/doc/man/man3/nvmlDeviceCommands.3
+++ b/nvidia-cuda/doc/man/man3/nvmlDeviceCommands.3
@@ -362,7 +362,7 @@ See \fBnvmlEnableState_t\fP for details
 .PP
 Set clocks that device will lock to.
 .PP
-Sets the clocks that the device will be running at to the value in the range of minGpuClockMHz to maxGpuClockMHz. Setting this will supercede application clock values and take effect regardless if a cuda app is running. See /ref nvmlDeviceSetApplicationsClocks
+Sets the clocks that the device will be running at to the value in the range of minGpuClockMHz to maxGpuClockMHz. Setting this will supersede application clock values and take effect regardless if a cuda app is running. See /ref nvmlDeviceSetApplicationsClocks
 .PP
 Can be used as a setting to request constant performance.
 .PP
--- a/nvidia-cuda/doc/man/man3/nvmlFBCStructs.3
+++ b/nvidia-cuda/doc/man/man3/nvmlFBCStructs.3
@@ -54,7 +54,7 @@ Represents frame buffer capture session
 .in +1c
 .TP
 \fB\fINVML_FBC_SESSION_TYPE_UNKNOWN \fP\fP
-Unknwon. 
+Unknown. 
 .TP
 \fB\fINVML_FBC_SESSION_TYPE_TOSYS \fP\fP
 ToSys. 
--- a/nvidia-cuda/doc/man/man3/nvmlSystemQueries.3
+++ b/nvidia-cuda/doc/man/man3/nvmlSystemQueries.3
@@ -45,7 +45,7 @@ Retrieves the version of the CUDA driver
 .PP
 For all products.
 .PP
-The CUDA driver version returned will be retreived from the currently installed version of CUDA. If the cuda library is not found, this function will return a known supported version number.
+The CUDA driver version returned will be retrieved from the currently installed version of CUDA. If the cuda library is not found, this function will return a known supported version number.
 .PP
 \fBParameters:\fP
 .RS 4
--- a/nvidia-cuda/doc/man/man3/nvmlVgpuPgpuMetadata_t.3
+++ b/nvidia-cuda/doc/man/man3/nvmlVgpuPgpuMetadata_t.3
@@ -26,7 +26,7 @@ nvmlVgpuPgpuMetadata_t \-
 .ti -1c
 .RI "unsigned int \fBpgpuVirtualizationCaps\fP"
 .br
-.RI "\fIPgpu virtualizaion capabilities bitfileld. \fP"
+.RI "\fIPgpu virtualization capabilities bitfield. \fP"
 .ti -1c
 .RI "unsigned int \fBreserved\fP [5]"
 .br
--- a/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__VA.html
+++ b/nvidia-cuda/doc/html/cuda-driver-api/group__CUDA__VA.html
@@ -443,7 +443,7 @@
                            </div>
                            <div class="section">
                               <h6 class="description_header">Description</h6>
-                              <p>This creates a memory allocation on the target device specified through the <tt class="ph tt code">prop</tt> strcuture. The created allocation will not have any device or host mappings. The generic memory <tt class="ph tt code">handle</tt> for the allocation can be mapped to the address space of calling process via <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1gff1d395423af5c5c75375516959dae56" title="Maps an allocation handle to a reserved virtual address range." shape="rect">cuMemMap</a>. This handle cannot be transmitted directly to other processes (see <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1g633f273b155815f23c1d70e7d9384c56" title="Exports an allocation to a requested shareable handle type." shape="rect">cuMemExportToShareableHandle</a>). On Windows, the caller must also pass an LPSECURITYATTRIBUTE in <tt class="ph tt code">prop</tt> to be associated with this handle which limits or allows access to this handle for a recepient process (see <a class="xref" href="structCUmemAllocationProp.html#structCUmemAllocationProp_16ff47d450fde73c3e5d46a01a6bf272d" shape="rect">CUmemAllocationProp::win32HandleMetaData</a> for more). The <tt class="ph tt code">size</tt> of this allocation must be a multiple of the the value given via <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1g30ee906c2cf66a0347b3dfec3d7eb31a" title="Calculates either the minimal or recommended granularity." shape="rect">cuMemGetAllocationGranularity</a> with the <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg3a202e4d32ae296db1af7efe75ce365dc74872d07341bb1ac24ccc4a1c9c2f56" shape="rect">CU_MEM_ALLOC_GRANULARITY_MINIMUM</a> flag.
+                              <p>This creates a memory allocation on the target device specified through the <tt class="ph tt code">prop</tt> structure. The created allocation will not have any device or host mappings. The generic memory <tt class="ph tt code">handle</tt> for the allocation can be mapped to the address space of calling process via <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1gff1d395423af5c5c75375516959dae56" title="Maps an allocation handle to a reserved virtual address range." shape="rect">cuMemMap</a>. This handle cannot be transmitted directly to other processes (see <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1g633f273b155815f23c1d70e7d9384c56" title="Exports an allocation to a requested shareable handle type." shape="rect">cuMemExportToShareableHandle</a>). On Windows, the caller must also pass an LPSECURITYATTRIBUTE in <tt class="ph tt code">prop</tt> to be associated with this handle which limits or allows access to this handle for a recipient process (see <a class="xref" href="structCUmemAllocationProp.html#structCUmemAllocationProp_16ff47d450fde73c3e5d46a01a6bf272d" shape="rect">CUmemAllocationProp::win32HandleMetaData</a> for more). The <tt class="ph tt code">size</tt> of this allocation must be a multiple of the the value given via <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1g30ee906c2cf66a0347b3dfec3d7eb31a" title="Calculates either the minimal or recommended granularity." shape="rect">cuMemGetAllocationGranularity</a> with the <a class="xref" href="group__CUDA__TYPES.html#group__CUDA__TYPES_1gg3a202e4d32ae296db1af7efe75ce365dc74872d07341bb1ac24ccc4a1c9c2f56" shape="rect">CU_MEM_ALLOC_GRANULARITY_MINIMUM</a> flag.
                               </p>
                               <p class="p"></p>
                               <p class="p"></p>
@@ -712,7 +712,7 @@
                               <p>Frees the memory that was allocated on a device through cuMemCreate.</p>
                               <p class="p">The memory allocation will be freed when all outstanding mappings to the memory are unmapped and when all outstanding references
                                  to the handle (including it's shareable counterparts) are also released. The generic memory handle can be freed when there
-                                 are still outstanding mappings made with this handle. Each time a recepient process imports a shareable handle, it needs to
+                                 are still outstanding mappings made with this handle. Each time a recipient process imports a shareable handle, it needs to
                                  pair it with <a class="xref" href="group__CUDA__VA.html#group__CUDA__VA_1g3014f0759f43a8d82db951b8e4b91d68" title="Release a memory handle representing a memory allocation which was previously allocated through cuMemCreate." shape="rect">cuMemRelease</a> for the handle to be freed. If <tt class="ph tt code">handle</tt> is not a valid handle the behavior is undefined.
                               </p>
                               <p class="p"></p>
--- a/nvidia-cuda/doc/man/man3/CUDA_VA.3
+++ b/nvidia-cuda/doc/man/man3/CUDA_VA.3
@@ -114,7 +114,7 @@ Reserves a virtual address range based o
 
 .SS "\fBCUresult\fP cuMemCreate (CUmemGenericAllocationHandle * handle, size_t size, const \fBCUmemAllocationProp\fP * prop, unsigned long long flags)"
 .PP
-This creates a memory allocation on the target device specified through the \fCprop\fP strcuture. The created allocation will not have any device or host mappings. The generic memory \fChandle\fP for the allocation can be mapped to the address space of calling process via \fBcuMemMap\fP. This handle cannot be transmitted directly to other processes (see \fBcuMemExportToShareableHandle\fP). On Windows, the caller must also pass an LPSECURITYATTRIBUTE in \fCprop\fP to be associated with this handle which limits or allows access to this handle for a recepient process (see \fBCUmemAllocationProp::win32HandleMetaData\fP for more). The \fCsize\fP of this allocation must be a multiple of the the value given via \fBcuMemGetAllocationGranularity\fP with the \fBCU_MEM_ALLOC_GRANULARITY_MINIMUM\fP flag.
+This creates a memory allocation on the target device specified through the \fCprop\fP structure. The created allocation will not have any device or host mappings. The generic memory \fChandle\fP for the allocation can be mapped to the address space of calling process via \fBcuMemMap\fP. This handle cannot be transmitted directly to other processes (see \fBcuMemExportToShareableHandle\fP). On Windows, the caller must also pass an LPSECURITYATTRIBUTE in \fCprop\fP to be associated with this handle which limits or allows access to this handle for a recipient process (see \fBCUmemAllocationProp::win32HandleMetaData\fP for more). The \fCsize\fP of this allocation must be a multiple of the the value given via \fBcuMemGetAllocationGranularity\fP with the \fBCU_MEM_ALLOC_GRANULARITY_MINIMUM\fP flag.
 .PP
 \fBParameters:\fP
 .RS 4
@@ -316,7 +316,7 @@ Note that this function may also return
 .PP
 Frees the memory that was allocated on a device through cuMemCreate.
 .PP
-The memory allocation will be freed when all outstanding mappings to the memory are unmapped and when all outstanding references to the handle (including it's shareable counterparts) are also released. The generic memory handle can be freed when there are still outstanding mappings made with this handle. Each time a recepient process imports a shareable handle, it needs to pair it with \fBcuMemRelease\fP for the handle to be freed. If \fChandle\fP is not a valid handle the behavior is undefined.
+The memory allocation will be freed when all outstanding mappings to the memory are unmapped and when all outstanding references to the handle (including it's shareable counterparts) are also released. The generic memory handle can be freed when there are still outstanding mappings made with this handle. Each time a recipient process imports a shareable handle, it needs to pair it with \fBcuMemRelease\fP for the handle to be freed. If \fChandle\fP is not a valid handle the behavior is undefined.
 .PP
 \fBParameters:\fP
 .RS 4
--- a/nvidia-cuda/doc/man/man3/nvmlVgpuMetadata_t.3
+++ b/nvidia-cuda/doc/man/man3/nvmlVgpuMetadata_t.3
@@ -38,7 +38,7 @@ nvmlVgpuMetadata_t \-
 .ti -1c
 .RI "unsigned int \fBvgpuVirtualizationCaps\fP"
 .br
-.RI "\fIvGPU virtualizaion capabilities bitfileld \fP"
+.RI "\fIvGPU virtualization capabilities bitfield \fP"
 .ti -1c
 .RI "unsigned int \fBguestVgpuVersion\fP"
 .br
--- a/nvidia-cuda/extras/CUPTI/samples/openacc_trace/openacc_trace.cpp
+++ b/nvidia-cuda/extras/CUPTI/samples/openacc_trace/openacc_trace.cpp
@@ -111,7 +111,7 @@ void finalize()
 }
 
 // acc_register_library is defined by the OpenACC tools interface
-// and allows to register this library with the OpenACC runtime.
+// and allows one to register this library with the OpenACC runtime.
 
 extern "C" void
 acc_register_library(void *profRegister, void *profUnregister, void *profLookup)
--- a/nvidia-cuda/include/cublasLt.h
+++ b/nvidia-cuda/include/cublasLt.h
@@ -589,7 +589,7 @@ cublasLtMatrixTransformDescGetAttribute(
     size_t sizeInBytes,
     size_t *sizeWritten);
 
-/** For computation with complex numbers, this enum allows to apply the Gauss Complexity reduction algorithm
+/** For computation with complex numbers, this enum allows one to apply the Gauss Complexity reduction algorithm
  */
 typedef enum {
     CUBLASLT_3M_MODE_DISALLOWED = 0,
--- a/nvidia-cuda/include/cusolverMg.h
+++ b/nvidia-cuda/include/cusolverMg.h
@@ -91,7 +91,7 @@ cusolverStatus_t CUSOLVERAPI cusolverMgD
 
 /**
  * \brief Allocates resources related to the shared memory device grid.
- * \param[out] grid the opaque data strcuture that holds the grid
+ * \param[out] grid the opaque data structure that holds the grid
  * \param[in] numRowDevices number of devices in the row
  * \param[in] numColDevices number of devices in the column
  * \param[in] deviceId This array of size height * width stores the
@@ -108,7 +108,7 @@ cusolverStatus_t CUSOLVERAPI cusolverMgC
 
 /**
  * \brief Releases the allocated resources related to the distributed grid.
- * \param[in] grid the opaque data strcuture that holds the distributed grid
+ * \param[in] grid the opaque data structure that holds the distributed grid
  * \returns the status code
  */
 cusolverStatus_t CUSOLVERAPI cusolverMgDestroyGrid(
@@ -116,7 +116,7 @@ cusolverStatus_t CUSOLVERAPI cusolverMgD
 
 /**
  * \brief Allocates resources related to the distributed matrix descriptor.
- * \param[out] desc the opaque data strcuture that holds the descriptor
+ * \param[out] desc the opaque data structure that holds the descriptor
  * \param[in] numRows number of total rows
  * \param[in] numCols number of total columns
  * \param[in] rowBlockSize row block size
@@ -136,7 +136,7 @@ cusolverStatus_t CUSOLVERAPI cusolverMgC
 
 /**
  * \brief Releases the allocated resources related to the distributed matrix descriptor.
- * \param[in] desc the opaque data strcuture that holds the descriptor
+ * \param[in] desc the opaque data structure that holds the descriptor
  * \returns the status code
  */
 cusolverStatus_t CUSOLVERAPI cusolverMgDestroyMatrixDesc(
